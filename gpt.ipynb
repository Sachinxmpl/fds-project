{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sachin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "batch_size = 64\n",
    "block_size = 128\n",
    "learning_rate = 3e-5\n",
    "max_itters = 3000\n",
    "eval_itters = 100\n",
    "n_embd = 384\n",
    "n_head = 8\n",
    "dropout = 0.2\n",
    "n_layer = 8\n",
    "epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "train_data = data[\"train\"]['text']\n",
    "test_data = data[\"validation\"]['text']\n",
    "\n",
    "train_text = \"\".join(train_data)\n",
    "test_text = \"\".join(test_data)\n",
    "\n",
    "chars = sorted(list(set(train_text + test_text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_string = {i: c for i, c in enumerate(chars)}\n",
    "encode = lambda x: [string_to_int[c] for c in x]\n",
    "decode = lambda x: \"\".join([int_to_string[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(encode(train_text)).to(device)\n",
    "test_data = torch.tensor(encode(test_text)).to(device)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else test_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(index)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            index_cond = index[:, -block_size:]\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        return index\n",
    "\n",
    "\n",
    "model = GPTLanguageModel(vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        losses = torch.zeros(eval_itters)\n",
    "        for k in range(eval_itters):\n",
    "            x, y = get_batch(split)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(chars)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# for i in range(epoch):\n",
    "#     with open('lenisha.pkl', 'rb') as f:\n",
    "#         model = pickle.load(f)\n",
    "#         print(\"Model loaded\")\n",
    "#     model.to(device)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     for itters in range(max_itters):\n",
    "#         if itters % eval_itters == 0:\n",
    "#             losses = estimate_loss()\n",
    "#             print(\n",
    "#                 f\"Iteration {i * 3000 + itters}, Train loss: {losses['train']:.4f}, Test loss: {losses['test']:.4f}\"\n",
    "#             )\n",
    "\n",
    "#         x, y = get_batch(\"train\")\n",
    "\n",
    "#         logits, loss = model.forward(x, y)\n",
    "\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     with open('lenisha.pkl', 'wb') as f:\n",
    "#         pickle.dump(model, f)\n",
    "#         print(\"Model saved\")\n",
    "        \n",
    "\n",
    "# print(\"Final loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Starting with fresh model\n",
      "Epoch 0, Iteration 0, Train loss: 7.0463, Test loss: 7.0480\n",
      "Epoch 0, Iteration 100, Train loss: 3.4989, Test loss: 3.5062\n",
      "Epoch 0, Iteration 200, Train loss: 2.9142, Test loss: 2.9191\n",
      "Epoch 0, Iteration 300, Train loss: 2.7119, Test loss: 2.7148\n",
      "Epoch 0, Iteration 400, Train loss: 2.6100, Test loss: 2.6118\n",
      "Epoch 0, Iteration 500, Train loss: 2.5574, Test loss: 2.5609\n",
      "Epoch 0, Iteration 600, Train loss: 2.5224, Test loss: 2.5235\n",
      "Epoch 0, Iteration 700, Train loss: 2.4972, Test loss: 2.4960\n",
      "Epoch 0, Iteration 800, Train loss: 2.4731, Test loss: 2.4770\n",
      "Epoch 0, Iteration 900, Train loss: 2.4496, Test loss: 2.4484\n",
      "Epoch 0, Iteration 1000, Train loss: 2.4134, Test loss: 2.4083\n",
      "Epoch 0, Iteration 1100, Train loss: 2.3642, Test loss: 2.3637\n",
      "Epoch 0, Iteration 1200, Train loss: 2.3159, Test loss: 2.3181\n",
      "Epoch 0, Iteration 1300, Train loss: 2.2791, Test loss: 2.2780\n",
      "Epoch 0, Iteration 1400, Train loss: 2.2375, Test loss: 2.2369\n",
      "Epoch 0, Iteration 1500, Train loss: 2.1998, Test loss: 2.1999\n",
      "Epoch 0, Iteration 1600, Train loss: 2.1758, Test loss: 2.1668\n",
      "Epoch 0, Iteration 1700, Train loss: 2.1435, Test loss: 2.1407\n",
      "Epoch 0, Iteration 1800, Train loss: 2.1174, Test loss: 2.1128\n",
      "Epoch 0, Iteration 1900, Train loss: 2.0990, Test loss: 2.0940\n",
      "Epoch 0, Iteration 2000, Train loss: 2.0730, Test loss: 2.0727\n",
      "Epoch 0, Iteration 2100, Train loss: 2.0558, Test loss: 2.0545\n",
      "Epoch 0, Iteration 2200, Train loss: 2.0400, Test loss: 2.0325\n",
      "Epoch 0, Iteration 2300, Train loss: 2.0229, Test loss: 2.0178\n",
      "Epoch 0, Iteration 2400, Train loss: 2.0051, Test loss: 1.9965\n",
      "Epoch 0, Iteration 2500, Train loss: 1.9907, Test loss: 1.9806\n",
      "Epoch 0, Iteration 2600, Train loss: 1.9716, Test loss: 1.9647\n",
      "Epoch 0, Iteration 2700, Train loss: 1.9559, Test loss: 1.9518\n",
      "Epoch 0, Iteration 2800, Train loss: 1.9452, Test loss: 1.9414\n",
      "Epoch 0, Iteration 2900, Train loss: 1.9305, Test loss: 1.9211\n",
      "Epoch 0: Model saved\n",
      "Epoch 1: Model loaded\n",
      "Epoch 1, Iteration 3000, Train loss: 1.9201, Test loss: 1.9147\n",
      "Epoch 1, Iteration 3100, Train loss: 1.9021, Test loss: 1.8981\n",
      "Epoch 1, Iteration 3200, Train loss: 1.8903, Test loss: 1.8858\n",
      "Epoch 1, Iteration 3300, Train loss: 1.8816, Test loss: 1.8734\n",
      "Epoch 1, Iteration 3400, Train loss: 1.8685, Test loss: 1.8642\n",
      "Epoch 1, Iteration 3500, Train loss: 1.8607, Test loss: 1.8505\n",
      "Epoch 1, Iteration 3600, Train loss: 1.8491, Test loss: 1.8398\n",
      "Epoch 1, Iteration 3700, Train loss: 1.8370, Test loss: 1.8287\n",
      "Epoch 1, Iteration 3800, Train loss: 1.8273, Test loss: 1.8191\n",
      "Epoch 1, Iteration 3900, Train loss: 1.8138, Test loss: 1.8101\n",
      "Epoch 1, Iteration 4000, Train loss: 1.8043, Test loss: 1.7985\n",
      "Epoch 1, Iteration 4100, Train loss: 1.7939, Test loss: 1.7898\n",
      "Epoch 1, Iteration 4200, Train loss: 1.7890, Test loss: 1.7849\n",
      "Epoch 1, Iteration 4300, Train loss: 1.7784, Test loss: 1.7733\n",
      "Epoch 1, Iteration 4400, Train loss: 1.7716, Test loss: 1.7679\n",
      "Epoch 1, Iteration 4500, Train loss: 1.7651, Test loss: 1.7590\n",
      "Epoch 1, Iteration 4600, Train loss: 1.7596, Test loss: 1.7485\n",
      "Epoch 1, Iteration 4700, Train loss: 1.7486, Test loss: 1.7460\n",
      "Epoch 1, Iteration 4800, Train loss: 1.7423, Test loss: 1.7396\n",
      "Epoch 1, Iteration 4900, Train loss: 1.7284, Test loss: 1.7295\n",
      "Epoch 1, Iteration 5000, Train loss: 1.7240, Test loss: 1.7204\n",
      "Epoch 1, Iteration 5100, Train loss: 1.7190, Test loss: 1.7141\n",
      "Epoch 1, Iteration 5200, Train loss: 1.7076, Test loss: 1.7035\n",
      "Epoch 1, Iteration 5300, Train loss: 1.7079, Test loss: 1.7006\n",
      "Epoch 1, Iteration 5400, Train loss: 1.6978, Test loss: 1.6974\n",
      "Epoch 1, Iteration 5500, Train loss: 1.6854, Test loss: 1.6811\n",
      "Epoch 1, Iteration 5600, Train loss: 1.6841, Test loss: 1.6808\n",
      "Epoch 1, Iteration 5700, Train loss: 1.6764, Test loss: 1.6779\n",
      "Epoch 1, Iteration 5800, Train loss: 1.6673, Test loss: 1.6680\n",
      "Epoch 1, Iteration 5900, Train loss: 1.6662, Test loss: 1.6607\n",
      "Epoch 1: Model saved\n",
      "Epoch 2: Model loaded\n",
      "Epoch 2, Iteration 6000, Train loss: 1.6622, Test loss: 1.6581\n",
      "Epoch 2, Iteration 6100, Train loss: 1.6538, Test loss: 1.6561\n",
      "Epoch 2, Iteration 6200, Train loss: 1.6476, Test loss: 1.6466\n",
      "Epoch 2, Iteration 6300, Train loss: 1.6463, Test loss: 1.6409\n",
      "Epoch 2, Iteration 6400, Train loss: 1.6403, Test loss: 1.6401\n",
      "Epoch 2, Iteration 6500, Train loss: 1.6390, Test loss: 1.6347\n",
      "Epoch 2, Iteration 6600, Train loss: 1.6343, Test loss: 1.6305\n",
      "Epoch 2, Iteration 6700, Train loss: 1.6289, Test loss: 1.6229\n",
      "Epoch 2, Iteration 6800, Train loss: 1.6223, Test loss: 1.6196\n",
      "Epoch 2, Iteration 6900, Train loss: 1.6139, Test loss: 1.6181\n",
      "Epoch 2, Iteration 7000, Train loss: 1.6112, Test loss: 1.6120\n",
      "Epoch 2, Iteration 7100, Train loss: 1.6045, Test loss: 1.6116\n",
      "Epoch 2, Iteration 7200, Train loss: 1.6055, Test loss: 1.6028\n",
      "Epoch 2, Iteration 7300, Train loss: 1.6004, Test loss: 1.5956\n",
      "Epoch 2, Iteration 7400, Train loss: 1.5915, Test loss: 1.5913\n",
      "Epoch 2, Iteration 7500, Train loss: 1.5890, Test loss: 1.5906\n",
      "Epoch 2, Iteration 7600, Train loss: 1.5830, Test loss: 1.5929\n",
      "Epoch 2, Iteration 7700, Train loss: 1.5835, Test loss: 1.5867\n",
      "Epoch 2, Iteration 7800, Train loss: 1.5811, Test loss: 1.5735\n",
      "Epoch 2, Iteration 7900, Train loss: 1.5696, Test loss: 1.5709\n",
      "Epoch 2, Iteration 8000, Train loss: 1.5693, Test loss: 1.5698\n",
      "Epoch 2, Iteration 8100, Train loss: 1.5677, Test loss: 1.5681\n",
      "Epoch 2, Iteration 8200, Train loss: 1.5659, Test loss: 1.5655\n",
      "Epoch 2, Iteration 8300, Train loss: 1.5559, Test loss: 1.5559\n",
      "Epoch 2, Iteration 8400, Train loss: 1.5572, Test loss: 1.5600\n",
      "Epoch 2, Iteration 8500, Train loss: 1.5548, Test loss: 1.5560\n",
      "Epoch 2, Iteration 8600, Train loss: 1.5485, Test loss: 1.5507\n",
      "Epoch 2, Iteration 8700, Train loss: 1.5473, Test loss: 1.5496\n",
      "Epoch 2, Iteration 8800, Train loss: 1.5434, Test loss: 1.5475\n",
      "Epoch 2, Iteration 8900, Train loss: 1.5424, Test loss: 1.5380\n",
      "Epoch 2: Model saved\n",
      "Epoch 3: Model loaded\n",
      "Epoch 3, Iteration 9000, Train loss: 1.5348, Test loss: 1.5399\n",
      "Epoch 3, Iteration 9100, Train loss: 1.5305, Test loss: 1.5314\n",
      "Epoch 3, Iteration 9200, Train loss: 1.5259, Test loss: 1.5294\n",
      "Epoch 3, Iteration 9300, Train loss: 1.5227, Test loss: 1.5326\n",
      "Epoch 3, Iteration 9400, Train loss: 1.5281, Test loss: 1.5281\n",
      "Epoch 3, Iteration 9500, Train loss: 1.5228, Test loss: 1.5281\n",
      "Epoch 3, Iteration 9600, Train loss: 1.5194, Test loss: 1.5207\n",
      "Epoch 3, Iteration 9700, Train loss: 1.5108, Test loss: 1.5165\n",
      "Epoch 3, Iteration 9800, Train loss: 1.5090, Test loss: 1.5146\n",
      "Epoch 3, Iteration 9900, Train loss: 1.5064, Test loss: 1.5146\n",
      "Epoch 3, Iteration 10000, Train loss: 1.5044, Test loss: 1.5169\n",
      "Epoch 3, Iteration 10100, Train loss: 1.5040, Test loss: 1.5090\n",
      "Epoch 3, Iteration 10200, Train loss: 1.4987, Test loss: 1.5072\n",
      "Epoch 3, Iteration 10300, Train loss: 1.4980, Test loss: 1.5030\n",
      "Epoch 3, Iteration 10400, Train loss: 1.4945, Test loss: 1.5029\n",
      "Epoch 3, Iteration 10500, Train loss: 1.4916, Test loss: 1.5004\n",
      "Epoch 3, Iteration 10600, Train loss: 1.4938, Test loss: 1.4998\n",
      "Epoch 3, Iteration 10700, Train loss: 1.4851, Test loss: 1.4967\n",
      "Epoch 3, Iteration 10800, Train loss: 1.4860, Test loss: 1.4898\n",
      "Epoch 3, Iteration 10900, Train loss: 1.4848, Test loss: 1.4921\n",
      "Epoch 3, Iteration 11000, Train loss: 1.4787, Test loss: 1.4882\n",
      "Epoch 3, Iteration 11100, Train loss: 1.4762, Test loss: 1.4918\n",
      "Epoch 3, Iteration 11200, Train loss: 1.4817, Test loss: 1.4891\n",
      "Epoch 3, Iteration 11300, Train loss: 1.4767, Test loss: 1.4872\n",
      "Epoch 3, Iteration 11400, Train loss: 1.4740, Test loss: 1.4844\n",
      "Epoch 3, Iteration 11500, Train loss: 1.4718, Test loss: 1.4795\n",
      "Epoch 3, Iteration 11600, Train loss: 1.4667, Test loss: 1.4804\n",
      "Epoch 3, Iteration 11700, Train loss: 1.4694, Test loss: 1.4814\n",
      "Epoch 3, Iteration 11800, Train loss: 1.4706, Test loss: 1.4738\n",
      "Epoch 3, Iteration 11900, Train loss: 1.4657, Test loss: 1.4762\n",
      "Epoch 3: Model saved\n",
      "Epoch 4: Model loaded\n",
      "Epoch 4, Iteration 12000, Train loss: 1.4563, Test loss: 1.4727\n",
      "Epoch 4, Iteration 12100, Train loss: 1.4584, Test loss: 1.4697\n",
      "Epoch 4, Iteration 12200, Train loss: 1.4571, Test loss: 1.4678\n",
      "Epoch 4, Iteration 12300, Train loss: 1.4534, Test loss: 1.4686\n",
      "Epoch 4, Iteration 12400, Train loss: 1.4475, Test loss: 1.4586\n",
      "Epoch 4, Iteration 12500, Train loss: 1.4520, Test loss: 1.4614\n",
      "Epoch 4, Iteration 12600, Train loss: 1.4471, Test loss: 1.4605\n",
      "Epoch 4, Iteration 12700, Train loss: 1.4474, Test loss: 1.4471\n",
      "Epoch 4, Iteration 12800, Train loss: 1.4461, Test loss: 1.4512\n",
      "Epoch 4, Iteration 12900, Train loss: 1.4381, Test loss: 1.4533\n",
      "Epoch 4, Iteration 13000, Train loss: 1.4416, Test loss: 1.4493\n",
      "Epoch 4, Iteration 13100, Train loss: 1.4454, Test loss: 1.4497\n",
      "Epoch 4, Iteration 13200, Train loss: 1.4337, Test loss: 1.4506\n",
      "Epoch 4, Iteration 13300, Train loss: 1.4356, Test loss: 1.4524\n",
      "Epoch 4, Iteration 13400, Train loss: 1.4304, Test loss: 1.4460\n",
      "Epoch 4, Iteration 13500, Train loss: 1.4303, Test loss: 1.4460\n",
      "Epoch 4, Iteration 13600, Train loss: 1.4286, Test loss: 1.4464\n",
      "Epoch 4, Iteration 13700, Train loss: 1.4292, Test loss: 1.4417\n",
      "Epoch 4, Iteration 13800, Train loss: 1.4276, Test loss: 1.4472\n",
      "Epoch 4, Iteration 13900, Train loss: 1.4285, Test loss: 1.4417\n",
      "Epoch 4, Iteration 14000, Train loss: 1.4278, Test loss: 1.4382\n",
      "Epoch 4, Iteration 14100, Train loss: 1.4244, Test loss: 1.4449\n",
      "Epoch 4, Iteration 14200, Train loss: 1.4220, Test loss: 1.4406\n",
      "Epoch 4, Iteration 14300, Train loss: 1.4190, Test loss: 1.4347\n",
      "Epoch 4, Iteration 14400, Train loss: 1.4187, Test loss: 1.4266\n",
      "Epoch 4, Iteration 14500, Train loss: 1.4222, Test loss: 1.4386\n",
      "Epoch 4, Iteration 14600, Train loss: 1.4142, Test loss: 1.4274\n",
      "Epoch 4, Iteration 14700, Train loss: 1.4121, Test loss: 1.4248\n",
      "Epoch 4, Iteration 14800, Train loss: 1.4116, Test loss: 1.4322\n",
      "Epoch 4, Iteration 14900, Train loss: 1.4161, Test loss: 1.4267\n",
      "Epoch 4: Model saved\n",
      "Epoch 5: Model loaded\n",
      "Epoch 5, Iteration 15000, Train loss: 1.4067, Test loss: 1.4273\n",
      "Epoch 5, Iteration 15100, Train loss: 1.4104, Test loss: 1.4276\n",
      "Epoch 5, Iteration 15200, Train loss: 1.4094, Test loss: 1.4290\n",
      "Epoch 5, Iteration 15300, Train loss: 1.4080, Test loss: 1.4251\n",
      "Epoch 5, Iteration 15400, Train loss: 1.4023, Test loss: 1.4199\n",
      "Epoch 5, Iteration 15500, Train loss: 1.4013, Test loss: 1.4226\n",
      "Epoch 5, Iteration 15600, Train loss: 1.4050, Test loss: 1.4228\n",
      "Epoch 5, Iteration 15700, Train loss: 1.4019, Test loss: 1.4225\n",
      "Epoch 5, Iteration 15800, Train loss: 1.3989, Test loss: 1.4171\n",
      "Epoch 5, Iteration 15900, Train loss: 1.3969, Test loss: 1.4175\n",
      "Epoch 5, Iteration 16000, Train loss: 1.4007, Test loss: 1.4092\n",
      "Epoch 5, Iteration 16100, Train loss: 1.3958, Test loss: 1.4094\n",
      "Epoch 5, Iteration 16200, Train loss: 1.3979, Test loss: 1.4162\n",
      "Epoch 5, Iteration 16300, Train loss: 1.3935, Test loss: 1.4117\n",
      "Epoch 5, Iteration 16400, Train loss: 1.3917, Test loss: 1.4154\n",
      "Epoch 5, Iteration 16500, Train loss: 1.3846, Test loss: 1.4160\n",
      "Epoch 5, Iteration 16600, Train loss: 1.3897, Test loss: 1.4135\n",
      "Epoch 5, Iteration 16700, Train loss: 1.3901, Test loss: 1.4104\n",
      "Epoch 5, Iteration 16800, Train loss: 1.3871, Test loss: 1.4143\n",
      "Epoch 5, Iteration 16900, Train loss: 1.3832, Test loss: 1.4115\n",
      "Epoch 5, Iteration 17000, Train loss: 1.3876, Test loss: 1.4026\n",
      "Epoch 5, Iteration 17100, Train loss: 1.3806, Test loss: 1.4007\n",
      "Epoch 5, Iteration 17200, Train loss: 1.3842, Test loss: 1.3988\n",
      "Epoch 5, Iteration 17300, Train loss: 1.3771, Test loss: 1.4019\n",
      "Epoch 5, Iteration 17400, Train loss: 1.3806, Test loss: 1.4023\n",
      "Epoch 5, Iteration 17500, Train loss: 1.3809, Test loss: 1.3978\n",
      "Epoch 5, Iteration 17600, Train loss: 1.3818, Test loss: 1.4028\n",
      "Epoch 5, Iteration 17700, Train loss: 1.3794, Test loss: 1.4054\n",
      "Epoch 5, Iteration 17800, Train loss: 1.3739, Test loss: 1.3968\n",
      "Epoch 5, Iteration 17900, Train loss: 1.3734, Test loss: 1.4003\n",
      "Epoch 5: Model saved\n",
      "Epoch 6: Model loaded\n",
      "Epoch 6, Iteration 18000, Train loss: 1.3743, Test loss: 1.3989\n",
      "Epoch 6, Iteration 18100, Train loss: 1.3721, Test loss: 1.3996\n",
      "Epoch 6, Iteration 18200, Train loss: 1.3750, Test loss: 1.3946\n",
      "Epoch 6, Iteration 18300, Train loss: 1.3671, Test loss: 1.3906\n",
      "Epoch 6, Iteration 18400, Train loss: 1.3722, Test loss: 1.3879\n",
      "Epoch 6, Iteration 18500, Train loss: 1.3698, Test loss: 1.3936\n",
      "Epoch 6, Iteration 18600, Train loss: 1.3697, Test loss: 1.3904\n",
      "Epoch 6, Iteration 18700, Train loss: 1.3695, Test loss: 1.3932\n",
      "Epoch 6, Iteration 18800, Train loss: 1.3682, Test loss: 1.3910\n",
      "Epoch 6, Iteration 18900, Train loss: 1.3708, Test loss: 1.3907\n",
      "Epoch 6, Iteration 19000, Train loss: 1.3603, Test loss: 1.3834\n",
      "Epoch 6, Iteration 19100, Train loss: 1.3616, Test loss: 1.3835\n",
      "Epoch 6, Iteration 19200, Train loss: 1.3647, Test loss: 1.3886\n",
      "Epoch 6, Iteration 19300, Train loss: 1.3592, Test loss: 1.3845\n",
      "Epoch 6, Iteration 19400, Train loss: 1.3622, Test loss: 1.3841\n",
      "Epoch 6, Iteration 19500, Train loss: 1.3586, Test loss: 1.3805\n",
      "Epoch 6, Iteration 19600, Train loss: 1.3642, Test loss: 1.3788\n",
      "Epoch 6, Iteration 19700, Train loss: 1.3549, Test loss: 1.3845\n",
      "Epoch 6, Iteration 19800, Train loss: 1.3531, Test loss: 1.3797\n",
      "Epoch 6, Iteration 19900, Train loss: 1.3544, Test loss: 1.3759\n",
      "Epoch 6, Iteration 20000, Train loss: 1.3539, Test loss: 1.3809\n",
      "Epoch 6, Iteration 20100, Train loss: 1.3550, Test loss: 1.3817\n",
      "Epoch 6, Iteration 20200, Train loss: 1.3538, Test loss: 1.3753\n",
      "Epoch 6, Iteration 20300, Train loss: 1.3496, Test loss: 1.3775\n",
      "Epoch 6, Iteration 20400, Train loss: 1.3505, Test loss: 1.3771\n",
      "Epoch 6, Iteration 20500, Train loss: 1.3476, Test loss: 1.3753\n",
      "Epoch 6, Iteration 20600, Train loss: 1.3456, Test loss: 1.3773\n",
      "Epoch 6, Iteration 20700, Train loss: 1.3478, Test loss: 1.3756\n",
      "Epoch 6, Iteration 20800, Train loss: 1.3473, Test loss: 1.3694\n",
      "Epoch 6, Iteration 20900, Train loss: 1.3442, Test loss: 1.3786\n",
      "Epoch 6: Model saved\n",
      "Epoch 7: Model loaded\n",
      "Epoch 7, Iteration 21000, Train loss: 1.3445, Test loss: 1.3789\n",
      "Epoch 7, Iteration 21100, Train loss: 1.3402, Test loss: 1.3722\n",
      "Epoch 7, Iteration 21200, Train loss: 1.3453, Test loss: 1.3725\n",
      "Epoch 7, Iteration 21300, Train loss: 1.3419, Test loss: 1.3743\n",
      "Epoch 7, Iteration 21400, Train loss: 1.3425, Test loss: 1.3703\n",
      "Epoch 7, Iteration 21500, Train loss: 1.3425, Test loss: 1.3646\n",
      "Epoch 7, Iteration 21600, Train loss: 1.3426, Test loss: 1.3730\n",
      "Epoch 7, Iteration 21700, Train loss: 1.3378, Test loss: 1.3693\n",
      "Epoch 7, Iteration 21800, Train loss: 1.3358, Test loss: 1.3690\n",
      "Epoch 7, Iteration 21900, Train loss: 1.3367, Test loss: 1.3634\n",
      "Epoch 7, Iteration 22000, Train loss: 1.3403, Test loss: 1.3630\n",
      "Epoch 7, Iteration 22100, Train loss: 1.3394, Test loss: 1.3658\n",
      "Epoch 7, Iteration 22200, Train loss: 1.3380, Test loss: 1.3661\n",
      "Epoch 7, Iteration 22300, Train loss: 1.3304, Test loss: 1.3622\n",
      "Epoch 7, Iteration 22400, Train loss: 1.3310, Test loss: 1.3584\n",
      "Epoch 7, Iteration 22500, Train loss: 1.3332, Test loss: 1.3620\n",
      "Epoch 7, Iteration 22600, Train loss: 1.3294, Test loss: 1.3640\n",
      "Epoch 7, Iteration 22700, Train loss: 1.3291, Test loss: 1.3631\n",
      "Epoch 7, Iteration 22800, Train loss: 1.3265, Test loss: 1.3611\n",
      "Epoch 7, Iteration 22900, Train loss: 1.3257, Test loss: 1.3588\n",
      "Epoch 7, Iteration 23000, Train loss: 1.3271, Test loss: 1.3635\n",
      "Epoch 7, Iteration 23100, Train loss: 1.3308, Test loss: 1.3593\n",
      "Epoch 7, Iteration 23200, Train loss: 1.3286, Test loss: 1.3602\n",
      "Epoch 7, Iteration 23300, Train loss: 1.3269, Test loss: 1.3579\n",
      "Epoch 7, Iteration 23400, Train loss: 1.3240, Test loss: 1.3631\n",
      "Epoch 7, Iteration 23500, Train loss: 1.3250, Test loss: 1.3560\n",
      "Epoch 7, Iteration 23600, Train loss: 1.3241, Test loss: 1.3589\n",
      "Epoch 7, Iteration 23700, Train loss: 1.3255, Test loss: 1.3558\n",
      "Epoch 7, Iteration 23800, Train loss: 1.3207, Test loss: 1.3542\n",
      "Epoch 7, Iteration 23900, Train loss: 1.3139, Test loss: 1.3566\n",
      "Epoch 7: Model saved\n",
      "Epoch 8: Model loaded\n",
      "Epoch 8, Iteration 24000, Train loss: 1.3286, Test loss: 1.3561\n",
      "Epoch 8, Iteration 24100, Train loss: 1.3257, Test loss: 1.3517\n",
      "Epoch 8, Iteration 24200, Train loss: 1.3188, Test loss: 1.3526\n",
      "Epoch 8, Iteration 24300, Train loss: 1.3223, Test loss: 1.3489\n",
      "Epoch 8, Iteration 24400, Train loss: 1.3152, Test loss: 1.3515\n",
      "Epoch 8, Iteration 24500, Train loss: 1.3211, Test loss: 1.3514\n",
      "Epoch 8, Iteration 24600, Train loss: 1.3150, Test loss: 1.3527\n",
      "Epoch 8, Iteration 24700, Train loss: 1.3191, Test loss: 1.3490\n",
      "Epoch 8, Iteration 24800, Train loss: 1.3170, Test loss: 1.3521\n",
      "Epoch 8, Iteration 24900, Train loss: 1.3150, Test loss: 1.3534\n",
      "Epoch 8, Iteration 25000, Train loss: 1.3137, Test loss: 1.3525\n",
      "Epoch 8, Iteration 25100, Train loss: 1.3135, Test loss: 1.3476\n",
      "Epoch 8, Iteration 25200, Train loss: 1.3119, Test loss: 1.3444\n",
      "Epoch 8, Iteration 25300, Train loss: 1.3152, Test loss: 1.3481\n",
      "Epoch 8, Iteration 25400, Train loss: 1.3128, Test loss: 1.3526\n",
      "Epoch 8, Iteration 25500, Train loss: 1.3063, Test loss: 1.3424\n",
      "Epoch 8, Iteration 25600, Train loss: 1.3111, Test loss: 1.3513\n",
      "Epoch 8, Iteration 25700, Train loss: 1.3107, Test loss: 1.3398\n",
      "Epoch 8, Iteration 25800, Train loss: 1.3132, Test loss: 1.3478\n",
      "Epoch 8, Iteration 25900, Train loss: 1.3095, Test loss: 1.3488\n",
      "Epoch 8, Iteration 26000, Train loss: 1.3106, Test loss: 1.3454\n",
      "Epoch 8, Iteration 26100, Train loss: 1.3109, Test loss: 1.3412\n",
      "Epoch 8, Iteration 26200, Train loss: 1.3054, Test loss: 1.3470\n",
      "Epoch 8, Iteration 26300, Train loss: 1.3068, Test loss: 1.3412\n",
      "Epoch 8, Iteration 26400, Train loss: 1.3052, Test loss: 1.3413\n",
      "Epoch 8, Iteration 26500, Train loss: 1.3018, Test loss: 1.3356\n",
      "Epoch 8, Iteration 26600, Train loss: 1.3055, Test loss: 1.3404\n",
      "Epoch 8, Iteration 26700, Train loss: 1.3086, Test loss: 1.3418\n",
      "Epoch 8, Iteration 26800, Train loss: 1.3015, Test loss: 1.3493\n",
      "Epoch 8, Iteration 26900, Train loss: 1.3023, Test loss: 1.3372\n",
      "Epoch 8: Model saved\n",
      "Epoch 9: Model loaded\n",
      "Epoch 9, Iteration 27000, Train loss: 1.3030, Test loss: 1.3404\n",
      "Epoch 9, Iteration 27100, Train loss: 1.3072, Test loss: 1.3450\n",
      "Epoch 9, Iteration 27200, Train loss: 1.3013, Test loss: 1.3380\n",
      "Epoch 9, Iteration 27300, Train loss: 1.3004, Test loss: 1.3384\n",
      "Epoch 9, Iteration 27400, Train loss: 1.3005, Test loss: 1.3433\n",
      "Epoch 9, Iteration 27500, Train loss: 1.2984, Test loss: 1.3374\n",
      "Epoch 9, Iteration 27600, Train loss: 1.3021, Test loss: 1.3380\n",
      "Epoch 9, Iteration 27700, Train loss: 1.2979, Test loss: 1.3358\n",
      "Epoch 9, Iteration 27800, Train loss: 1.2930, Test loss: 1.3414\n",
      "Epoch 9, Iteration 27900, Train loss: 1.2944, Test loss: 1.3347\n",
      "Epoch 9, Iteration 28000, Train loss: 1.2982, Test loss: 1.3387\n",
      "Epoch 9, Iteration 28100, Train loss: 1.2934, Test loss: 1.3352\n",
      "Epoch 9, Iteration 28200, Train loss: 1.2969, Test loss: 1.3304\n",
      "Epoch 9, Iteration 28300, Train loss: 1.2943, Test loss: 1.3309\n",
      "Epoch 9, Iteration 28400, Train loss: 1.2934, Test loss: 1.3330\n",
      "Epoch 9, Iteration 28500, Train loss: 1.2867, Test loss: 1.3274\n",
      "Epoch 9, Iteration 28600, Train loss: 1.2905, Test loss: 1.3326\n",
      "Epoch 9, Iteration 28700, Train loss: 1.2964, Test loss: 1.3294\n",
      "Epoch 9, Iteration 28800, Train loss: 1.2919, Test loss: 1.3321\n",
      "Epoch 9, Iteration 28900, Train loss: 1.2966, Test loss: 1.3348\n",
      "Epoch 9, Iteration 29000, Train loss: 1.2915, Test loss: 1.3322\n",
      "Epoch 9, Iteration 29100, Train loss: 1.2916, Test loss: 1.3298\n",
      "Epoch 9, Iteration 29200, Train loss: 1.2926, Test loss: 1.3318\n",
      "Epoch 9, Iteration 29300, Train loss: 1.2858, Test loss: 1.3326\n",
      "Epoch 9, Iteration 29400, Train loss: 1.2868, Test loss: 1.3307\n",
      "Epoch 9, Iteration 29500, Train loss: 1.2912, Test loss: 1.3384\n",
      "Epoch 9, Iteration 29600, Train loss: 1.2854, Test loss: 1.3256\n",
      "Epoch 9, Iteration 29700, Train loss: 1.2871, Test loss: 1.3300\n",
      "Epoch 9, Iteration 29800, Train loss: 1.2889, Test loss: 1.3256\n",
      "Epoch 9, Iteration 29900, Train loss: 1.2859, Test loss: 1.3259\n",
      "Epoch 9: Model saved\n",
      "Epoch 10: Model loaded\n",
      "Epoch 10, Iteration 30000, Train loss: 1.2867, Test loss: 1.3317\n",
      "Epoch 10, Iteration 30100, Train loss: 1.2822, Test loss: 1.3262\n",
      "Epoch 10, Iteration 30200, Train loss: 1.2843, Test loss: 1.3292\n",
      "Epoch 10, Iteration 30300, Train loss: 1.2835, Test loss: 1.3243\n",
      "Epoch 10, Iteration 30400, Train loss: 1.2845, Test loss: 1.3278\n",
      "Epoch 10, Iteration 30500, Train loss: 1.2812, Test loss: 1.3334\n",
      "Epoch 10, Iteration 30600, Train loss: 1.2849, Test loss: 1.3223\n",
      "Epoch 10, Iteration 30700, Train loss: 1.2800, Test loss: 1.3260\n",
      "Epoch 10, Iteration 30800, Train loss: 1.2865, Test loss: 1.3207\n",
      "Epoch 10, Iteration 30900, Train loss: 1.2803, Test loss: 1.3188\n",
      "Epoch 10, Iteration 31000, Train loss: 1.2786, Test loss: 1.3217\n",
      "Epoch 10, Iteration 31100, Train loss: 1.2796, Test loss: 1.3266\n",
      "Epoch 10, Iteration 31200, Train loss: 1.2750, Test loss: 1.3205\n",
      "Epoch 10, Iteration 31300, Train loss: 1.2818, Test loss: 1.3248\n",
      "Epoch 10, Iteration 31400, Train loss: 1.2763, Test loss: 1.3201\n",
      "Epoch 10, Iteration 31500, Train loss: 1.2843, Test loss: 1.3236\n",
      "Epoch 10, Iteration 31600, Train loss: 1.2789, Test loss: 1.3247\n",
      "Epoch 10, Iteration 31700, Train loss: 1.2810, Test loss: 1.3191\n",
      "Epoch 10, Iteration 31800, Train loss: 1.2741, Test loss: 1.3176\n",
      "Epoch 10, Iteration 31900, Train loss: 1.2759, Test loss: 1.3200\n",
      "Epoch 10, Iteration 32000, Train loss: 1.2785, Test loss: 1.3178\n",
      "Epoch 10, Iteration 32100, Train loss: 1.2765, Test loss: 1.3138\n",
      "Epoch 10, Iteration 32200, Train loss: 1.2751, Test loss: 1.3136\n",
      "Epoch 10, Iteration 32300, Train loss: 1.2708, Test loss: 1.3179\n",
      "Epoch 10, Iteration 32400, Train loss: 1.2685, Test loss: 1.3164\n",
      "Epoch 10, Iteration 32500, Train loss: 1.2722, Test loss: 1.3162\n",
      "Epoch 10, Iteration 32600, Train loss: 1.2759, Test loss: 1.3214\n",
      "Epoch 10, Iteration 32700, Train loss: 1.2723, Test loss: 1.3144\n",
      "Epoch 10, Iteration 32800, Train loss: 1.2698, Test loss: 1.3253\n",
      "Epoch 10, Iteration 32900, Train loss: 1.2681, Test loss: 1.3214\n",
      "Epoch 10: Model saved\n",
      "Epoch 11: Model loaded\n",
      "Epoch 11, Iteration 33000, Train loss: 1.2690, Test loss: 1.3164\n",
      "Epoch 11, Iteration 33100, Train loss: 1.2705, Test loss: 1.3199\n",
      "Epoch 11, Iteration 33200, Train loss: 1.2688, Test loss: 1.3181\n",
      "Epoch 11, Iteration 33300, Train loss: 1.2750, Test loss: 1.3161\n",
      "Epoch 11, Iteration 33400, Train loss: 1.2677, Test loss: 1.3113\n",
      "Epoch 11, Iteration 33500, Train loss: 1.2717, Test loss: 1.3157\n",
      "Epoch 11, Iteration 33600, Train loss: 1.2687, Test loss: 1.3175\n",
      "Epoch 11, Iteration 33700, Train loss: 1.2644, Test loss: 1.3145\n",
      "Epoch 11, Iteration 33800, Train loss: 1.2647, Test loss: 1.3193\n",
      "Epoch 11, Iteration 33900, Train loss: 1.2733, Test loss: 1.3198\n",
      "Epoch 11, Iteration 34000, Train loss: 1.2679, Test loss: 1.3085\n",
      "Epoch 11, Iteration 34100, Train loss: 1.2700, Test loss: 1.3136\n",
      "Epoch 11, Iteration 34200, Train loss: 1.2649, Test loss: 1.3097\n",
      "Epoch 11, Iteration 34300, Train loss: 1.2676, Test loss: 1.3102\n",
      "Epoch 11, Iteration 34400, Train loss: 1.2614, Test loss: 1.3111\n",
      "Epoch 11, Iteration 34500, Train loss: 1.2627, Test loss: 1.3119\n",
      "Epoch 11, Iteration 34600, Train loss: 1.2613, Test loss: 1.3078\n",
      "Epoch 11, Iteration 34700, Train loss: 1.2608, Test loss: 1.3058\n",
      "Epoch 11, Iteration 34800, Train loss: 1.2637, Test loss: 1.3171\n",
      "Epoch 11, Iteration 34900, Train loss: 1.2597, Test loss: 1.3110\n",
      "Epoch 11, Iteration 35000, Train loss: 1.2605, Test loss: 1.3140\n",
      "Epoch 11, Iteration 35100, Train loss: 1.2632, Test loss: 1.3134\n",
      "Epoch 11, Iteration 35200, Train loss: 1.2616, Test loss: 1.3103\n",
      "Epoch 11, Iteration 35300, Train loss: 1.2619, Test loss: 1.3111\n",
      "Epoch 11, Iteration 35400, Train loss: 1.2630, Test loss: 1.3100\n",
      "Epoch 11, Iteration 35500, Train loss: 1.2617, Test loss: 1.3102\n",
      "Epoch 11, Iteration 35600, Train loss: 1.2609, Test loss: 1.3072\n",
      "Epoch 11, Iteration 35700, Train loss: 1.2563, Test loss: 1.3030\n",
      "Epoch 11, Iteration 35800, Train loss: 1.2557, Test loss: 1.3019\n",
      "Epoch 11, Iteration 35900, Train loss: 1.2559, Test loss: 1.3106\n",
      "Epoch 11: Model saved\n",
      "Epoch 12: Model loaded\n",
      "Epoch 12, Iteration 36000, Train loss: 1.2541, Test loss: 1.3144\n",
      "Epoch 12, Iteration 36100, Train loss: 1.2537, Test loss: 1.3072\n",
      "Epoch 12, Iteration 36200, Train loss: 1.2538, Test loss: 1.3115\n",
      "Epoch 12, Iteration 36300, Train loss: 1.2594, Test loss: 1.3095\n",
      "Epoch 12, Iteration 36400, Train loss: 1.2559, Test loss: 1.3095\n",
      "Epoch 12, Iteration 36500, Train loss: 1.2587, Test loss: 1.3058\n",
      "Epoch 12, Iteration 36600, Train loss: 1.2536, Test loss: 1.3010\n",
      "Epoch 12, Iteration 36700, Train loss: 1.2537, Test loss: 1.3051\n",
      "Epoch 12, Iteration 36800, Train loss: 1.2549, Test loss: 1.3071\n",
      "Epoch 12, Iteration 36900, Train loss: 1.2512, Test loss: 1.3070\n",
      "Epoch 12, Iteration 37000, Train loss: 1.2540, Test loss: 1.3101\n",
      "Epoch 12, Iteration 37100, Train loss: 1.2493, Test loss: 1.3023\n",
      "Epoch 12, Iteration 37200, Train loss: 1.2490, Test loss: 1.3038\n",
      "Epoch 12, Iteration 37300, Train loss: 1.2527, Test loss: 1.3050\n",
      "Epoch 12, Iteration 37400, Train loss: 1.2535, Test loss: 1.3028\n",
      "Epoch 12, Iteration 37500, Train loss: 1.2498, Test loss: 1.3025\n",
      "Epoch 12, Iteration 37600, Train loss: 1.2500, Test loss: 1.3090\n",
      "Epoch 12, Iteration 37700, Train loss: 1.2514, Test loss: 1.3067\n",
      "Epoch 12, Iteration 37800, Train loss: 1.2491, Test loss: 1.3086\n",
      "Epoch 12, Iteration 37900, Train loss: 1.2502, Test loss: 1.3045\n",
      "Epoch 12, Iteration 38000, Train loss: 1.2503, Test loss: 1.3027\n",
      "Epoch 12, Iteration 38100, Train loss: 1.2463, Test loss: 1.3043\n",
      "Epoch 12, Iteration 38200, Train loss: 1.2455, Test loss: 1.2998\n",
      "Epoch 12, Iteration 38300, Train loss: 1.2488, Test loss: 1.2967\n",
      "Epoch 12, Iteration 38400, Train loss: 1.2494, Test loss: 1.3004\n",
      "Epoch 12, Iteration 38500, Train loss: 1.2468, Test loss: 1.2983\n",
      "Epoch 12, Iteration 38600, Train loss: 1.2461, Test loss: 1.2970\n",
      "Epoch 12, Iteration 38700, Train loss: 1.2470, Test loss: 1.3053\n",
      "Epoch 12, Iteration 38800, Train loss: 1.2487, Test loss: 1.3001\n",
      "Epoch 12, Iteration 38900, Train loss: 1.2452, Test loss: 1.3000\n",
      "Epoch 12: Model saved\n",
      "Epoch 13: Model loaded\n",
      "Epoch 13, Iteration 39000, Train loss: 1.2495, Test loss: 1.3007\n",
      "Epoch 13, Iteration 39100, Train loss: 1.2444, Test loss: 1.2935\n",
      "Epoch 13, Iteration 39200, Train loss: 1.2453, Test loss: 1.3001\n",
      "Epoch 13, Iteration 39300, Train loss: 1.2473, Test loss: 1.3016\n",
      "Epoch 13, Iteration 39400, Train loss: 1.2470, Test loss: 1.3003\n",
      "Epoch 13, Iteration 39500, Train loss: 1.2409, Test loss: 1.3013\n",
      "Epoch 13, Iteration 39600, Train loss: 1.2451, Test loss: 1.2941\n",
      "Epoch 13, Iteration 39700, Train loss: 1.2458, Test loss: 1.2954\n",
      "Epoch 13, Iteration 39800, Train loss: 1.2409, Test loss: 1.3042\n",
      "Epoch 13, Iteration 39900, Train loss: 1.2369, Test loss: 1.2976\n",
      "Epoch 13, Iteration 40000, Train loss: 1.2420, Test loss: 1.2938\n",
      "Epoch 13, Iteration 40100, Train loss: 1.2369, Test loss: 1.2981\n",
      "Epoch 13, Iteration 40200, Train loss: 1.2417, Test loss: 1.2925\n",
      "Epoch 13, Iteration 40300, Train loss: 1.2400, Test loss: 1.2959\n",
      "Epoch 13, Iteration 40400, Train loss: 1.2334, Test loss: 1.2973\n",
      "Epoch 13, Iteration 40500, Train loss: 1.2374, Test loss: 1.2928\n",
      "Epoch 13, Iteration 40600, Train loss: 1.2445, Test loss: 1.2954\n",
      "Epoch 13, Iteration 40700, Train loss: 1.2364, Test loss: 1.2978\n",
      "Epoch 13, Iteration 40800, Train loss: 1.2397, Test loss: 1.2925\n",
      "Epoch 13, Iteration 40900, Train loss: 1.2383, Test loss: 1.2992\n",
      "Epoch 13, Iteration 41000, Train loss: 1.2397, Test loss: 1.2970\n",
      "Epoch 13, Iteration 41100, Train loss: 1.2436, Test loss: 1.2958\n",
      "Epoch 13, Iteration 41200, Train loss: 1.2410, Test loss: 1.2987\n",
      "Epoch 13, Iteration 41300, Train loss: 1.2350, Test loss: 1.2883\n",
      "Epoch 13, Iteration 41400, Train loss: 1.2390, Test loss: 1.2966\n",
      "Epoch 13, Iteration 41500, Train loss: 1.2342, Test loss: 1.2956\n",
      "Epoch 13, Iteration 41600, Train loss: 1.2322, Test loss: 1.2943\n",
      "Epoch 13, Iteration 41700, Train loss: 1.2318, Test loss: 1.2931\n",
      "Epoch 13, Iteration 41800, Train loss: 1.2332, Test loss: 1.2896\n",
      "Epoch 13, Iteration 41900, Train loss: 1.2351, Test loss: 1.2949\n",
      "Epoch 13: Model saved\n",
      "Epoch 14: Model loaded\n",
      "Epoch 14, Iteration 42000, Train loss: 1.2376, Test loss: 1.2929\n",
      "Epoch 14, Iteration 42100, Train loss: 1.2309, Test loss: 1.2949\n",
      "Epoch 14, Iteration 42200, Train loss: 1.2334, Test loss: 1.2950\n",
      "Epoch 14, Iteration 42300, Train loss: 1.2336, Test loss: 1.2914\n",
      "Epoch 14, Iteration 42400, Train loss: 1.2360, Test loss: 1.2927\n",
      "Epoch 14, Iteration 42500, Train loss: 1.2325, Test loss: 1.2943\n",
      "Epoch 14, Iteration 42600, Train loss: 1.2327, Test loss: 1.2898\n",
      "Epoch 14, Iteration 42700, Train loss: 1.2298, Test loss: 1.2848\n",
      "Epoch 14, Iteration 42800, Train loss: 1.2305, Test loss: 1.2920\n",
      "Epoch 14, Iteration 42900, Train loss: 1.2306, Test loss: 1.2908\n",
      "Epoch 14, Iteration 43000, Train loss: 1.2338, Test loss: 1.2904\n",
      "Epoch 14, Iteration 43100, Train loss: 1.2306, Test loss: 1.2937\n",
      "Epoch 14, Iteration 43200, Train loss: 1.2265, Test loss: 1.2843\n",
      "Epoch 14, Iteration 43300, Train loss: 1.2287, Test loss: 1.2904\n",
      "Epoch 14, Iteration 43400, Train loss: 1.2339, Test loss: 1.2887\n",
      "Epoch 14, Iteration 43500, Train loss: 1.2328, Test loss: 1.2903\n",
      "Epoch 14, Iteration 43600, Train loss: 1.2318, Test loss: 1.2892\n",
      "Epoch 14, Iteration 43700, Train loss: 1.2289, Test loss: 1.2928\n",
      "Epoch 14, Iteration 43800, Train loss: 1.2321, Test loss: 1.2906\n",
      "Epoch 14, Iteration 43900, Train loss: 1.2264, Test loss: 1.2901\n",
      "Epoch 14, Iteration 44000, Train loss: 1.2279, Test loss: 1.2876\n",
      "Epoch 14, Iteration 44100, Train loss: 1.2267, Test loss: 1.2830\n",
      "Epoch 14, Iteration 44200, Train loss: 1.2246, Test loss: 1.2835\n",
      "Epoch 14, Iteration 44300, Train loss: 1.2290, Test loss: 1.2920\n",
      "Epoch 14, Iteration 44400, Train loss: 1.2317, Test loss: 1.2871\n",
      "Epoch 14, Iteration 44500, Train loss: 1.2267, Test loss: 1.2836\n",
      "Epoch 14, Iteration 44600, Train loss: 1.2263, Test loss: 1.2866\n",
      "Epoch 14, Iteration 44700, Train loss: 1.2281, Test loss: 1.2860\n",
      "Epoch 14, Iteration 44800, Train loss: 1.2266, Test loss: 1.2835\n",
      "Epoch 14, Iteration 44900, Train loss: 1.2234, Test loss: 1.2877\n",
      "Epoch 14: Model saved\n",
      "Epoch 15: Model loaded\n",
      "Epoch 15, Iteration 45000, Train loss: 1.2271, Test loss: 1.2890\n",
      "Epoch 15, Iteration 45100, Train loss: 1.2239, Test loss: 1.2871\n",
      "Epoch 15, Iteration 45200, Train loss: 1.2244, Test loss: 1.2790\n",
      "Epoch 15, Iteration 45300, Train loss: 1.2222, Test loss: 1.2869\n",
      "Epoch 15, Iteration 45400, Train loss: 1.2203, Test loss: 1.2815\n",
      "Epoch 15, Iteration 45500, Train loss: 1.2243, Test loss: 1.2886\n",
      "Epoch 15, Iteration 45600, Train loss: 1.2231, Test loss: 1.2838\n",
      "Epoch 15, Iteration 45700, Train loss: 1.2175, Test loss: 1.2851\n",
      "Epoch 15, Iteration 45800, Train loss: 1.2218, Test loss: 1.2878\n",
      "Epoch 15, Iteration 45900, Train loss: 1.2215, Test loss: 1.2828\n",
      "Epoch 15, Iteration 46000, Train loss: 1.2212, Test loss: 1.2843\n",
      "Epoch 15, Iteration 46100, Train loss: 1.2195, Test loss: 1.2839\n",
      "Epoch 15, Iteration 46200, Train loss: 1.2206, Test loss: 1.2810\n",
      "Epoch 15, Iteration 46300, Train loss: 1.2228, Test loss: 1.2864\n",
      "Epoch 15, Iteration 46400, Train loss: 1.2206, Test loss: 1.2845\n",
      "Epoch 15, Iteration 46500, Train loss: 1.2197, Test loss: 1.2839\n",
      "Epoch 15, Iteration 46600, Train loss: 1.2166, Test loss: 1.2831\n",
      "Epoch 15, Iteration 46700, Train loss: 1.2181, Test loss: 1.2792\n",
      "Epoch 15, Iteration 46800, Train loss: 1.2237, Test loss: 1.2843\n",
      "Epoch 15, Iteration 46900, Train loss: 1.2189, Test loss: 1.2810\n",
      "Epoch 15, Iteration 47000, Train loss: 1.2160, Test loss: 1.2818\n",
      "Epoch 15, Iteration 47100, Train loss: 1.2232, Test loss: 1.2837\n",
      "Epoch 15, Iteration 47200, Train loss: 1.2140, Test loss: 1.2840\n",
      "Epoch 15, Iteration 47300, Train loss: 1.2148, Test loss: 1.2821\n",
      "Epoch 15, Iteration 47400, Train loss: 1.2165, Test loss: 1.2831\n",
      "Epoch 15, Iteration 47500, Train loss: 1.2149, Test loss: 1.2773\n",
      "Epoch 15, Iteration 47600, Train loss: 1.2192, Test loss: 1.2814\n",
      "Epoch 15, Iteration 47700, Train loss: 1.2159, Test loss: 1.2785\n",
      "Epoch 15, Iteration 47800, Train loss: 1.2199, Test loss: 1.2806\n",
      "Epoch 15, Iteration 47900, Train loss: 1.2176, Test loss: 1.2754\n",
      "Epoch 15: Model saved\n",
      "Epoch 16: Model loaded\n",
      "Epoch 16, Iteration 48000, Train loss: 1.2111, Test loss: 1.2747\n",
      "Epoch 16, Iteration 48100, Train loss: 1.2149, Test loss: 1.2802\n",
      "Epoch 16, Iteration 48200, Train loss: 1.2168, Test loss: 1.2860\n",
      "Epoch 16, Iteration 48300, Train loss: 1.2188, Test loss: 1.2821\n",
      "Epoch 16, Iteration 48400, Train loss: 1.2170, Test loss: 1.2832\n",
      "Epoch 16, Iteration 48500, Train loss: 1.2130, Test loss: 1.2751\n",
      "Epoch 16, Iteration 48600, Train loss: 1.2105, Test loss: 1.2760\n",
      "Epoch 16, Iteration 48700, Train loss: 1.2078, Test loss: 1.2740\n",
      "Epoch 16, Iteration 48800, Train loss: 1.2153, Test loss: 1.2723\n",
      "Epoch 16, Iteration 48900, Train loss: 1.2126, Test loss: 1.2793\n",
      "Epoch 16, Iteration 49000, Train loss: 1.2134, Test loss: 1.2780\n",
      "Epoch 16, Iteration 49100, Train loss: 1.2101, Test loss: 1.2709\n",
      "Epoch 16, Iteration 49200, Train loss: 1.2125, Test loss: 1.2761\n",
      "Epoch 16, Iteration 49300, Train loss: 1.2134, Test loss: 1.2762\n",
      "Epoch 16, Iteration 49400, Train loss: 1.2093, Test loss: 1.2709\n",
      "Epoch 16, Iteration 49500, Train loss: 1.2100, Test loss: 1.2784\n",
      "Epoch 16, Iteration 49600, Train loss: 1.2059, Test loss: 1.2741\n",
      "Epoch 16, Iteration 49700, Train loss: 1.2112, Test loss: 1.2755\n",
      "Epoch 16, Iteration 49800, Train loss: 1.2110, Test loss: 1.2774\n",
      "Epoch 16, Iteration 49900, Train loss: 1.2134, Test loss: 1.2757\n",
      "Epoch 16, Iteration 50000, Train loss: 1.2136, Test loss: 1.2740\n",
      "Epoch 16, Iteration 50100, Train loss: 1.2104, Test loss: 1.2750\n",
      "Epoch 16, Iteration 50200, Train loss: 1.2078, Test loss: 1.2746\n",
      "Epoch 16, Iteration 50300, Train loss: 1.2063, Test loss: 1.2723\n",
      "Epoch 16, Iteration 50400, Train loss: 1.2099, Test loss: 1.2772\n",
      "Epoch 16, Iteration 50500, Train loss: 1.2079, Test loss: 1.2736\n",
      "Epoch 16, Iteration 50600, Train loss: 1.2056, Test loss: 1.2812\n",
      "Epoch 16, Iteration 50700, Train loss: 1.2101, Test loss: 1.2769\n",
      "Epoch 16, Iteration 50800, Train loss: 1.2054, Test loss: 1.2783\n",
      "Epoch 16, Iteration 50900, Train loss: 1.2063, Test loss: 1.2713\n",
      "Epoch 16: Model saved\n",
      "Epoch 17: Model loaded\n",
      "Epoch 17, Iteration 51000, Train loss: 1.2057, Test loss: 1.2732\n",
      "Epoch 17, Iteration 51100, Train loss: 1.2022, Test loss: 1.2712\n",
      "Epoch 17, Iteration 51200, Train loss: 1.2066, Test loss: 1.2751\n",
      "Epoch 17, Iteration 51300, Train loss: 1.2068, Test loss: 1.2779\n",
      "Epoch 17, Iteration 51400, Train loss: 1.2056, Test loss: 1.2724\n",
      "Epoch 17, Iteration 51500, Train loss: 1.2035, Test loss: 1.2724\n",
      "Epoch 17, Iteration 51600, Train loss: 1.2042, Test loss: 1.2785\n",
      "Epoch 17, Iteration 51700, Train loss: 1.2032, Test loss: 1.2740\n",
      "Epoch 17, Iteration 51800, Train loss: 1.2018, Test loss: 1.2719\n",
      "Epoch 17, Iteration 51900, Train loss: 1.2077, Test loss: 1.2809\n",
      "Epoch 17, Iteration 52000, Train loss: 1.2024, Test loss: 1.2716\n",
      "Epoch 17, Iteration 52100, Train loss: 1.2020, Test loss: 1.2729\n",
      "Epoch 17, Iteration 52200, Train loss: 1.2004, Test loss: 1.2710\n",
      "Epoch 17, Iteration 52300, Train loss: 1.2044, Test loss: 1.2709\n",
      "Epoch 17, Iteration 52400, Train loss: 1.2030, Test loss: 1.2733\n",
      "Epoch 17, Iteration 52500, Train loss: 1.2004, Test loss: 1.2646\n",
      "Epoch 17, Iteration 52600, Train loss: 1.1994, Test loss: 1.2717\n",
      "Epoch 17, Iteration 52700, Train loss: 1.2047, Test loss: 1.2749\n",
      "Epoch 17, Iteration 52800, Train loss: 1.2043, Test loss: 1.2707\n",
      "Epoch 17, Iteration 52900, Train loss: 1.2069, Test loss: 1.2683\n",
      "Epoch 17, Iteration 53000, Train loss: 1.2073, Test loss: 1.2723\n",
      "Epoch 17, Iteration 53100, Train loss: 1.2004, Test loss: 1.2719\n",
      "Epoch 17, Iteration 53200, Train loss: 1.2004, Test loss: 1.2732\n",
      "Epoch 17, Iteration 53300, Train loss: 1.1967, Test loss: 1.2733\n",
      "Epoch 17, Iteration 53400, Train loss: 1.1983, Test loss: 1.2686\n",
      "Epoch 17, Iteration 53500, Train loss: 1.1948, Test loss: 1.2710\n",
      "Epoch 17, Iteration 53600, Train loss: 1.1960, Test loss: 1.2707\n",
      "Epoch 17, Iteration 53700, Train loss: 1.1984, Test loss: 1.2734\n",
      "Epoch 17, Iteration 53800, Train loss: 1.2000, Test loss: 1.2712\n",
      "Epoch 17, Iteration 53900, Train loss: 1.2001, Test loss: 1.2790\n",
      "Epoch 17: Model saved\n",
      "Epoch 18: Model loaded\n",
      "Epoch 18, Iteration 54000, Train loss: 1.2031, Test loss: 1.2710\n",
      "Epoch 18, Iteration 54100, Train loss: 1.1984, Test loss: 1.2724\n",
      "Epoch 18, Iteration 54200, Train loss: 1.1965, Test loss: 1.2715\n",
      "Epoch 18, Iteration 54300, Train loss: 1.2006, Test loss: 1.2710\n",
      "Epoch 18, Iteration 54400, Train loss: 1.1955, Test loss: 1.2643\n",
      "Epoch 18, Iteration 54500, Train loss: 1.1989, Test loss: 1.2720\n",
      "Epoch 18, Iteration 54600, Train loss: 1.1962, Test loss: 1.2695\n",
      "Epoch 18, Iteration 54700, Train loss: 1.1969, Test loss: 1.2688\n",
      "Epoch 18, Iteration 54800, Train loss: 1.1932, Test loss: 1.2667\n",
      "Epoch 18, Iteration 54900, Train loss: 1.1933, Test loss: 1.2698\n",
      "Epoch 18, Iteration 55000, Train loss: 1.1977, Test loss: 1.2665\n",
      "Epoch 18, Iteration 55100, Train loss: 1.1955, Test loss: 1.2648\n",
      "Epoch 18, Iteration 55200, Train loss: 1.1934, Test loss: 1.2714\n",
      "Epoch 18, Iteration 55300, Train loss: 1.2013, Test loss: 1.2630\n",
      "Epoch 18, Iteration 55400, Train loss: 1.1954, Test loss: 1.2665\n",
      "Epoch 18, Iteration 55500, Train loss: 1.1968, Test loss: 1.2712\n",
      "Epoch 18, Iteration 55600, Train loss: 1.1898, Test loss: 1.2679\n",
      "Epoch 18, Iteration 55700, Train loss: 1.1948, Test loss: 1.2626\n",
      "Epoch 18, Iteration 55800, Train loss: 1.1914, Test loss: 1.2628\n",
      "Epoch 18, Iteration 55900, Train loss: 1.1940, Test loss: 1.2620\n",
      "Epoch 18, Iteration 56000, Train loss: 1.1933, Test loss: 1.2665\n",
      "Epoch 18, Iteration 56100, Train loss: 1.1916, Test loss: 1.2648\n",
      "Epoch 18, Iteration 56200, Train loss: 1.1956, Test loss: 1.2698\n",
      "Epoch 18, Iteration 56300, Train loss: 1.1899, Test loss: 1.2623\n",
      "Epoch 18, Iteration 56400, Train loss: 1.1946, Test loss: 1.2608\n",
      "Epoch 18, Iteration 56500, Train loss: 1.1985, Test loss: 1.2677\n",
      "Epoch 18, Iteration 56600, Train loss: 1.1919, Test loss: 1.2646\n",
      "Epoch 18, Iteration 56700, Train loss: 1.1934, Test loss: 1.2703\n",
      "Epoch 18, Iteration 56800, Train loss: 1.1918, Test loss: 1.2671\n",
      "Epoch 18, Iteration 56900, Train loss: 1.1939, Test loss: 1.2651\n",
      "Epoch 18: Model saved\n",
      "Epoch 19: Model loaded\n",
      "Epoch 19, Iteration 57000, Train loss: 1.1889, Test loss: 1.2654\n",
      "Epoch 19, Iteration 57100, Train loss: 1.1938, Test loss: 1.2618\n",
      "Epoch 19, Iteration 57200, Train loss: 1.1895, Test loss: 1.2626\n",
      "Epoch 19, Iteration 57300, Train loss: 1.1945, Test loss: 1.2684\n",
      "Epoch 19, Iteration 57400, Train loss: 1.1904, Test loss: 1.2655\n",
      "Epoch 19, Iteration 57500, Train loss: 1.1932, Test loss: 1.2616\n",
      "Epoch 19, Iteration 57600, Train loss: 1.1867, Test loss: 1.2626\n",
      "Epoch 19, Iteration 57700, Train loss: 1.1888, Test loss: 1.2673\n",
      "Epoch 19, Iteration 57800, Train loss: 1.1874, Test loss: 1.2627\n",
      "Epoch 19, Iteration 57900, Train loss: 1.1934, Test loss: 1.2591\n",
      "Epoch 19, Iteration 58000, Train loss: 1.1912, Test loss: 1.2604\n",
      "Epoch 19, Iteration 58100, Train loss: 1.1872, Test loss: 1.2630\n",
      "Epoch 19, Iteration 58200, Train loss: 1.1869, Test loss: 1.2671\n",
      "Epoch 19, Iteration 58300, Train loss: 1.1900, Test loss: 1.2608\n",
      "Epoch 19, Iteration 58400, Train loss: 1.1882, Test loss: 1.2637\n",
      "Epoch 19, Iteration 58500, Train loss: 1.1873, Test loss: 1.2639\n",
      "Epoch 19, Iteration 58600, Train loss: 1.1902, Test loss: 1.2631\n",
      "Epoch 19, Iteration 58700, Train loss: 1.1866, Test loss: 1.2602\n",
      "Epoch 19, Iteration 58800, Train loss: 1.1909, Test loss: 1.2642\n",
      "Epoch 19, Iteration 58900, Train loss: 1.1868, Test loss: 1.2610\n",
      "Epoch 19, Iteration 59000, Train loss: 1.1858, Test loss: 1.2610\n",
      "Epoch 19, Iteration 59100, Train loss: 1.1840, Test loss: 1.2578\n",
      "Epoch 19, Iteration 59200, Train loss: 1.1885, Test loss: 1.2620\n",
      "Epoch 19, Iteration 59300, Train loss: 1.1843, Test loss: 1.2616\n",
      "Epoch 19, Iteration 59400, Train loss: 1.1846, Test loss: 1.2585\n",
      "Epoch 19, Iteration 59500, Train loss: 1.1912, Test loss: 1.2615\n",
      "Epoch 19, Iteration 59600, Train loss: 1.1852, Test loss: 1.2568\n",
      "Epoch 19, Iteration 59700, Train loss: 1.1839, Test loss: 1.2622\n",
      "Epoch 19, Iteration 59800, Train loss: 1.1818, Test loss: 1.2579\n",
      "Epoch 19, Iteration 59900, Train loss: 1.1857, Test loss: 1.2644\n",
      "Epoch 19: Model saved\n",
      "Epoch 20: Model loaded\n",
      "Epoch 20, Iteration 60000, Train loss: 1.1864, Test loss: 1.2613\n",
      "Epoch 20, Iteration 60100, Train loss: 1.1887, Test loss: 1.2621\n",
      "Epoch 20, Iteration 60200, Train loss: 1.1843, Test loss: 1.2674\n",
      "Epoch 20, Iteration 60300, Train loss: 1.1830, Test loss: 1.2563\n",
      "Epoch 20, Iteration 60400, Train loss: 1.1797, Test loss: 1.2611\n",
      "Epoch 20, Iteration 60500, Train loss: 1.1809, Test loss: 1.2558\n",
      "Epoch 20, Iteration 60600, Train loss: 1.1809, Test loss: 1.2587\n",
      "Epoch 20, Iteration 60700, Train loss: 1.1872, Test loss: 1.2559\n",
      "Epoch 20, Iteration 60800, Train loss: 1.1818, Test loss: 1.2558\n",
      "Epoch 20, Iteration 60900, Train loss: 1.1845, Test loss: 1.2582\n",
      "Epoch 20, Iteration 61000, Train loss: 1.1826, Test loss: 1.2576\n",
      "Epoch 20, Iteration 61100, Train loss: 1.1826, Test loss: 1.2618\n",
      "Epoch 20, Iteration 61200, Train loss: 1.1815, Test loss: 1.2581\n",
      "Epoch 20, Iteration 61300, Train loss: 1.1825, Test loss: 1.2563\n",
      "Epoch 20, Iteration 61400, Train loss: 1.1813, Test loss: 1.2576\n",
      "Epoch 20, Iteration 61500, Train loss: 1.1783, Test loss: 1.2598\n",
      "Epoch 20, Iteration 61600, Train loss: 1.1766, Test loss: 1.2572\n",
      "Epoch 20, Iteration 61700, Train loss: 1.1804, Test loss: 1.2557\n",
      "Epoch 20, Iteration 61800, Train loss: 1.1822, Test loss: 1.2612\n",
      "Epoch 20, Iteration 61900, Train loss: 1.1756, Test loss: 1.2546\n",
      "Epoch 20, Iteration 62000, Train loss: 1.1760, Test loss: 1.2636\n",
      "Epoch 20, Iteration 62100, Train loss: 1.1790, Test loss: 1.2630\n",
      "Epoch 20, Iteration 62200, Train loss: 1.1768, Test loss: 1.2584\n",
      "Epoch 20, Iteration 62300, Train loss: 1.1791, Test loss: 1.2619\n",
      "Epoch 20, Iteration 62400, Train loss: 1.1759, Test loss: 1.2543\n",
      "Epoch 20, Iteration 62500, Train loss: 1.1782, Test loss: 1.2584\n",
      "Epoch 20, Iteration 62600, Train loss: 1.1732, Test loss: 1.2638\n",
      "Epoch 20, Iteration 62700, Train loss: 1.1768, Test loss: 1.2557\n",
      "Epoch 20, Iteration 62800, Train loss: 1.1778, Test loss: 1.2517\n",
      "Epoch 20, Iteration 62900, Train loss: 1.1741, Test loss: 1.2602\n",
      "Epoch 20: Model saved\n",
      "Epoch 21: Model loaded\n",
      "Epoch 21, Iteration 63000, Train loss: 1.1805, Test loss: 1.2614\n",
      "Epoch 21, Iteration 63100, Train loss: 1.1810, Test loss: 1.2547\n",
      "Epoch 21, Iteration 63200, Train loss: 1.1750, Test loss: 1.2576\n",
      "Epoch 21, Iteration 63300, Train loss: 1.1783, Test loss: 1.2554\n",
      "Epoch 21, Iteration 63400, Train loss: 1.1749, Test loss: 1.2538\n",
      "Epoch 21, Iteration 63500, Train loss: 1.1779, Test loss: 1.2532\n",
      "Epoch 21, Iteration 63600, Train loss: 1.1722, Test loss: 1.2547\n",
      "Epoch 21, Iteration 63700, Train loss: 1.1772, Test loss: 1.2575\n",
      "Epoch 21, Iteration 63800, Train loss: 1.1741, Test loss: 1.2490\n",
      "Epoch 21, Iteration 63900, Train loss: 1.1765, Test loss: 1.2571\n",
      "Epoch 21, Iteration 64000, Train loss: 1.1789, Test loss: 1.2486\n",
      "Epoch 21, Iteration 64100, Train loss: 1.1750, Test loss: 1.2596\n",
      "Epoch 21, Iteration 64200, Train loss: 1.1777, Test loss: 1.2571\n",
      "Epoch 21, Iteration 64300, Train loss: 1.1776, Test loss: 1.2534\n",
      "Epoch 21, Iteration 64400, Train loss: 1.1759, Test loss: 1.2500\n",
      "Epoch 21, Iteration 64500, Train loss: 1.1739, Test loss: 1.2572\n",
      "Epoch 21, Iteration 64600, Train loss: 1.1723, Test loss: 1.2494\n",
      "Epoch 21, Iteration 64700, Train loss: 1.1775, Test loss: 1.2515\n",
      "Epoch 21, Iteration 64800, Train loss: 1.1786, Test loss: 1.2499\n",
      "Epoch 21, Iteration 64900, Train loss: 1.1720, Test loss: 1.2523\n",
      "Epoch 21, Iteration 65000, Train loss: 1.1747, Test loss: 1.2541\n",
      "Epoch 21, Iteration 65100, Train loss: 1.1735, Test loss: 1.2559\n",
      "Epoch 21, Iteration 65200, Train loss: 1.1721, Test loss: 1.2531\n",
      "Epoch 21, Iteration 65300, Train loss: 1.1686, Test loss: 1.2503\n",
      "Epoch 21, Iteration 65400, Train loss: 1.1728, Test loss: 1.2570\n",
      "Epoch 21, Iteration 65500, Train loss: 1.1705, Test loss: 1.2473\n",
      "Epoch 21, Iteration 65600, Train loss: 1.1718, Test loss: 1.2489\n",
      "Epoch 21, Iteration 65700, Train loss: 1.1675, Test loss: 1.2527\n",
      "Epoch 21, Iteration 65800, Train loss: 1.1724, Test loss: 1.2487\n",
      "Epoch 21, Iteration 65900, Train loss: 1.1716, Test loss: 1.2529\n",
      "Epoch 21: Model saved\n",
      "Epoch 22: Model loaded\n",
      "Epoch 22, Iteration 66000, Train loss: 1.1690, Test loss: 1.2527\n",
      "Epoch 22, Iteration 66100, Train loss: 1.1719, Test loss: 1.2523\n",
      "Epoch 22, Iteration 66200, Train loss: 1.1728, Test loss: 1.2526\n",
      "Epoch 22, Iteration 66300, Train loss: 1.1737, Test loss: 1.2554\n",
      "Epoch 22, Iteration 66400, Train loss: 1.1701, Test loss: 1.2581\n",
      "Epoch 22, Iteration 66500, Train loss: 1.1691, Test loss: 1.2542\n",
      "Epoch 22, Iteration 66600, Train loss: 1.1698, Test loss: 1.2502\n",
      "Epoch 22, Iteration 66700, Train loss: 1.1700, Test loss: 1.2526\n",
      "Epoch 22, Iteration 66800, Train loss: 1.1678, Test loss: 1.2478\n",
      "Epoch 22, Iteration 66900, Train loss: 1.1697, Test loss: 1.2465\n",
      "Epoch 22, Iteration 67000, Train loss: 1.1682, Test loss: 1.2516\n",
      "Epoch 22, Iteration 67100, Train loss: 1.1674, Test loss: 1.2471\n",
      "Epoch 22, Iteration 67200, Train loss: 1.1665, Test loss: 1.2519\n",
      "Epoch 22, Iteration 67300, Train loss: 1.1706, Test loss: 1.2504\n",
      "Epoch 22, Iteration 67400, Train loss: 1.1717, Test loss: 1.2480\n",
      "Epoch 22, Iteration 67500, Train loss: 1.1653, Test loss: 1.2508\n",
      "Epoch 22, Iteration 67600, Train loss: 1.1705, Test loss: 1.2530\n",
      "Epoch 22, Iteration 67700, Train loss: 1.1686, Test loss: 1.2536\n",
      "Epoch 22, Iteration 67800, Train loss: 1.1668, Test loss: 1.2533\n",
      "Epoch 22, Iteration 67900, Train loss: 1.1693, Test loss: 1.2522\n",
      "Epoch 22, Iteration 68000, Train loss: 1.1671, Test loss: 1.2517\n",
      "Epoch 22, Iteration 68100, Train loss: 1.1635, Test loss: 1.2468\n",
      "Epoch 22, Iteration 68200, Train loss: 1.1668, Test loss: 1.2517\n",
      "Epoch 22, Iteration 68300, Train loss: 1.1659, Test loss: 1.2525\n",
      "Epoch 22, Iteration 68400, Train loss: 1.1661, Test loss: 1.2490\n",
      "Epoch 22, Iteration 68500, Train loss: 1.1669, Test loss: 1.2492\n",
      "Epoch 22, Iteration 68600, Train loss: 1.1614, Test loss: 1.2467\n",
      "Epoch 22, Iteration 68700, Train loss: 1.1670, Test loss: 1.2452\n",
      "Epoch 22, Iteration 68800, Train loss: 1.1631, Test loss: 1.2522\n",
      "Epoch 22, Iteration 68900, Train loss: 1.1658, Test loss: 1.2503\n",
      "Epoch 22: Model saved\n",
      "Epoch 23: Model loaded\n",
      "Epoch 23, Iteration 69000, Train loss: 1.1630, Test loss: 1.2487\n",
      "Epoch 23, Iteration 69100, Train loss: 1.1681, Test loss: 1.2526\n",
      "Epoch 23, Iteration 69200, Train loss: 1.1680, Test loss: 1.2484\n",
      "Epoch 23, Iteration 69300, Train loss: 1.1587, Test loss: 1.2482\n",
      "Epoch 23, Iteration 69400, Train loss: 1.1656, Test loss: 1.2480\n",
      "Epoch 23, Iteration 69500, Train loss: 1.1613, Test loss: 1.2423\n",
      "Epoch 23, Iteration 69600, Train loss: 1.1658, Test loss: 1.2442\n",
      "Epoch 23, Iteration 69700, Train loss: 1.1635, Test loss: 1.2481\n",
      "Epoch 23, Iteration 69800, Train loss: 1.1665, Test loss: 1.2509\n",
      "Epoch 23, Iteration 69900, Train loss: 1.1618, Test loss: 1.2458\n",
      "Epoch 23, Iteration 70000, Train loss: 1.1623, Test loss: 1.2490\n",
      "Epoch 23, Iteration 70100, Train loss: 1.1620, Test loss: 1.2428\n",
      "Epoch 23, Iteration 70200, Train loss: 1.1616, Test loss: 1.2482\n",
      "Epoch 23, Iteration 70300, Train loss: 1.1614, Test loss: 1.2452\n",
      "Epoch 23, Iteration 70400, Train loss: 1.1606, Test loss: 1.2482\n",
      "Epoch 23, Iteration 70500, Train loss: 1.1626, Test loss: 1.2478\n",
      "Epoch 23, Iteration 70600, Train loss: 1.1649, Test loss: 1.2479\n",
      "Epoch 23, Iteration 70700, Train loss: 1.1614, Test loss: 1.2492\n",
      "Epoch 23, Iteration 70800, Train loss: 1.1615, Test loss: 1.2487\n",
      "Epoch 23, Iteration 70900, Train loss: 1.1607, Test loss: 1.2520\n",
      "Epoch 23, Iteration 71000, Train loss: 1.1598, Test loss: 1.2502\n",
      "Epoch 23, Iteration 71100, Train loss: 1.1580, Test loss: 1.2445\n",
      "Epoch 23, Iteration 71200, Train loss: 1.1591, Test loss: 1.2442\n",
      "Epoch 23, Iteration 71300, Train loss: 1.1592, Test loss: 1.2435\n",
      "Epoch 23, Iteration 71400, Train loss: 1.1626, Test loss: 1.2465\n",
      "Epoch 23, Iteration 71500, Train loss: 1.1585, Test loss: 1.2457\n",
      "Epoch 23, Iteration 71600, Train loss: 1.1605, Test loss: 1.2453\n",
      "Epoch 23, Iteration 71700, Train loss: 1.1558, Test loss: 1.2481\n",
      "Epoch 23, Iteration 71800, Train loss: 1.1570, Test loss: 1.2481\n",
      "Epoch 23, Iteration 71900, Train loss: 1.1625, Test loss: 1.2436\n",
      "Epoch 23: Model saved\n",
      "Epoch 24: Model loaded\n",
      "Epoch 24, Iteration 72000, Train loss: 1.1616, Test loss: 1.2425\n",
      "Epoch 24, Iteration 72100, Train loss: 1.1569, Test loss: 1.2490\n",
      "Epoch 24, Iteration 72200, Train loss: 1.1570, Test loss: 1.2452\n",
      "Epoch 24, Iteration 72300, Train loss: 1.1577, Test loss: 1.2488\n",
      "Epoch 24, Iteration 72400, Train loss: 1.1564, Test loss: 1.2458\n",
      "Epoch 24, Iteration 72500, Train loss: 1.1617, Test loss: 1.2466\n",
      "Epoch 24, Iteration 72600, Train loss: 1.1599, Test loss: 1.2448\n",
      "Epoch 24, Iteration 72700, Train loss: 1.1571, Test loss: 1.2456\n",
      "Epoch 24, Iteration 72800, Train loss: 1.1516, Test loss: 1.2443\n",
      "Epoch 24, Iteration 72900, Train loss: 1.1550, Test loss: 1.2453\n",
      "Epoch 24, Iteration 73000, Train loss: 1.1556, Test loss: 1.2434\n",
      "Epoch 24, Iteration 73100, Train loss: 1.1612, Test loss: 1.2416\n",
      "Epoch 24, Iteration 73200, Train loss: 1.1544, Test loss: 1.2409\n",
      "Epoch 24, Iteration 73300, Train loss: 1.1595, Test loss: 1.2437\n",
      "Epoch 24, Iteration 73400, Train loss: 1.1516, Test loss: 1.2441\n",
      "Epoch 24, Iteration 73500, Train loss: 1.1556, Test loss: 1.2432\n",
      "Epoch 24, Iteration 73600, Train loss: 1.1586, Test loss: 1.2470\n",
      "Epoch 24, Iteration 73700, Train loss: 1.1602, Test loss: 1.2526\n",
      "Epoch 24, Iteration 73800, Train loss: 1.1600, Test loss: 1.2480\n",
      "Epoch 24, Iteration 73900, Train loss: 1.1544, Test loss: 1.2414\n",
      "Epoch 24, Iteration 74000, Train loss: 1.1563, Test loss: 1.2414\n",
      "Epoch 24, Iteration 74100, Train loss: 1.1512, Test loss: 1.2468\n",
      "Epoch 24, Iteration 74200, Train loss: 1.1518, Test loss: 1.2429\n",
      "Epoch 24, Iteration 74300, Train loss: 1.1515, Test loss: 1.2444\n",
      "Epoch 24, Iteration 74400, Train loss: 1.1512, Test loss: 1.2429\n",
      "Epoch 24, Iteration 74500, Train loss: 1.1558, Test loss: 1.2388\n",
      "Epoch 24, Iteration 74600, Train loss: 1.1499, Test loss: 1.2396\n",
      "Epoch 24, Iteration 74700, Train loss: 1.1565, Test loss: 1.2492\n",
      "Epoch 24, Iteration 74800, Train loss: 1.1545, Test loss: 1.2393\n",
      "Epoch 24, Iteration 74900, Train loss: 1.1538, Test loss: 1.2427\n",
      "Epoch 24: Model saved\n",
      "Epoch 25: Model loaded\n",
      "Epoch 25, Iteration 75000, Train loss: 1.1549, Test loss: 1.2435\n",
      "Epoch 25, Iteration 75100, Train loss: 1.1563, Test loss: 1.2399\n",
      "Epoch 25, Iteration 75200, Train loss: 1.1535, Test loss: 1.2409\n",
      "Epoch 25, Iteration 75300, Train loss: 1.1532, Test loss: 1.2445\n",
      "Epoch 25, Iteration 75400, Train loss: 1.1538, Test loss: 1.2361\n",
      "Epoch 25, Iteration 75500, Train loss: 1.1505, Test loss: 1.2476\n",
      "Epoch 25, Iteration 75600, Train loss: 1.1533, Test loss: 1.2397\n",
      "Epoch 25, Iteration 75700, Train loss: 1.1534, Test loss: 1.2457\n",
      "Epoch 25, Iteration 75800, Train loss: 1.1502, Test loss: 1.2380\n",
      "Epoch 25, Iteration 75900, Train loss: 1.1535, Test loss: 1.2451\n",
      "Epoch 25, Iteration 76000, Train loss: 1.1487, Test loss: 1.2369\n",
      "Epoch 25, Iteration 76100, Train loss: 1.1487, Test loss: 1.2389\n",
      "Epoch 25, Iteration 76200, Train loss: 1.1498, Test loss: 1.2358\n",
      "Epoch 25, Iteration 76300, Train loss: 1.1491, Test loss: 1.2355\n",
      "Epoch 25, Iteration 76400, Train loss: 1.1511, Test loss: 1.2417\n",
      "Epoch 25, Iteration 76500, Train loss: 1.1513, Test loss: 1.2362\n",
      "Epoch 25, Iteration 76600, Train loss: 1.1506, Test loss: 1.2438\n",
      "Epoch 25, Iteration 76700, Train loss: 1.1477, Test loss: 1.2391\n",
      "Epoch 25, Iteration 76800, Train loss: 1.1514, Test loss: 1.2421\n",
      "Epoch 25, Iteration 76900, Train loss: 1.1513, Test loss: 1.2393\n",
      "Epoch 25, Iteration 77000, Train loss: 1.1494, Test loss: 1.2362\n",
      "Epoch 25, Iteration 77100, Train loss: 1.1498, Test loss: 1.2414\n",
      "Epoch 25, Iteration 77200, Train loss: 1.1475, Test loss: 1.2365\n",
      "Epoch 25, Iteration 77300, Train loss: 1.1471, Test loss: 1.2401\n",
      "Epoch 25, Iteration 77400, Train loss: 1.1505, Test loss: 1.2386\n",
      "Epoch 25, Iteration 77500, Train loss: 1.1486, Test loss: 1.2425\n",
      "Epoch 25, Iteration 77600, Train loss: 1.1485, Test loss: 1.2386\n",
      "Epoch 25, Iteration 77700, Train loss: 1.1453, Test loss: 1.2334\n",
      "Epoch 25, Iteration 77800, Train loss: 1.1478, Test loss: 1.2422\n",
      "Epoch 25, Iteration 77900, Train loss: 1.1499, Test loss: 1.2420\n",
      "Epoch 25: Model saved\n",
      "Epoch 26: Model loaded\n",
      "Epoch 26, Iteration 78000, Train loss: 1.1475, Test loss: 1.2376\n",
      "Epoch 26, Iteration 78100, Train loss: 1.1484, Test loss: 1.2411\n",
      "Epoch 26, Iteration 78200, Train loss: 1.1464, Test loss: 1.2376\n",
      "Epoch 26, Iteration 78300, Train loss: 1.1426, Test loss: 1.2359\n",
      "Epoch 26, Iteration 78400, Train loss: 1.1499, Test loss: 1.2419\n",
      "Epoch 26, Iteration 78500, Train loss: 1.1480, Test loss: 1.2391\n",
      "Epoch 26, Iteration 78600, Train loss: 1.1487, Test loss: 1.2460\n",
      "Epoch 26, Iteration 78700, Train loss: 1.1441, Test loss: 1.2407\n",
      "Epoch 26, Iteration 78800, Train loss: 1.1432, Test loss: 1.2364\n",
      "Epoch 26, Iteration 78900, Train loss: 1.1470, Test loss: 1.2391\n",
      "Epoch 26, Iteration 79000, Train loss: 1.1462, Test loss: 1.2390\n",
      "Epoch 26, Iteration 79100, Train loss: 1.1479, Test loss: 1.2399\n",
      "Epoch 26, Iteration 79200, Train loss: 1.1455, Test loss: 1.2402\n",
      "Epoch 26, Iteration 79300, Train loss: 1.1423, Test loss: 1.2354\n",
      "Epoch 26, Iteration 79400, Train loss: 1.1422, Test loss: 1.2409\n",
      "Epoch 26, Iteration 79500, Train loss: 1.1473, Test loss: 1.2420\n",
      "Epoch 26, Iteration 79600, Train loss: 1.1435, Test loss: 1.2312\n",
      "Epoch 26, Iteration 79700, Train loss: 1.1462, Test loss: 1.2435\n",
      "Epoch 26, Iteration 79800, Train loss: 1.1448, Test loss: 1.2378\n",
      "Epoch 26, Iteration 79900, Train loss: 1.1446, Test loss: 1.2403\n",
      "Epoch 26, Iteration 80000, Train loss: 1.1455, Test loss: 1.2384\n",
      "Epoch 26, Iteration 80100, Train loss: 1.1443, Test loss: 1.2333\n",
      "Epoch 26, Iteration 80200, Train loss: 1.1442, Test loss: 1.2427\n",
      "Epoch 26, Iteration 80300, Train loss: 1.1456, Test loss: 1.2390\n",
      "Epoch 26, Iteration 80400, Train loss: 1.1442, Test loss: 1.2373\n",
      "Epoch 26, Iteration 80500, Train loss: 1.1382, Test loss: 1.2398\n",
      "Epoch 26, Iteration 80600, Train loss: 1.1416, Test loss: 1.2309\n",
      "Epoch 26, Iteration 80700, Train loss: 1.1382, Test loss: 1.2355\n",
      "Epoch 26, Iteration 80800, Train loss: 1.1411, Test loss: 1.2366\n",
      "Epoch 26, Iteration 80900, Train loss: 1.1444, Test loss: 1.2361\n",
      "Epoch 26: Model saved\n",
      "Epoch 27: Model loaded\n",
      "Epoch 27, Iteration 81000, Train loss: 1.1414, Test loss: 1.2382\n",
      "Epoch 27, Iteration 81100, Train loss: 1.1434, Test loss: 1.2373\n",
      "Epoch 27, Iteration 81200, Train loss: 1.1466, Test loss: 1.2341\n",
      "Epoch 27, Iteration 81300, Train loss: 1.1353, Test loss: 1.2367\n",
      "Epoch 27, Iteration 81400, Train loss: 1.1422, Test loss: 1.2333\n",
      "Epoch 27, Iteration 81500, Train loss: 1.1372, Test loss: 1.2367\n",
      "Epoch 27, Iteration 81600, Train loss: 1.1449, Test loss: 1.2301\n",
      "Epoch 27, Iteration 81700, Train loss: 1.1418, Test loss: 1.2351\n",
      "Epoch 27, Iteration 81800, Train loss: 1.1398, Test loss: 1.2361\n",
      "Epoch 27, Iteration 81900, Train loss: 1.1431, Test loss: 1.2387\n",
      "Epoch 27, Iteration 82000, Train loss: 1.1431, Test loss: 1.2341\n",
      "Epoch 27, Iteration 82100, Train loss: 1.1422, Test loss: 1.2363\n",
      "Epoch 27, Iteration 82200, Train loss: 1.1400, Test loss: 1.2352\n",
      "Epoch 27, Iteration 82300, Train loss: 1.1435, Test loss: 1.2419\n",
      "Epoch 27, Iteration 82400, Train loss: 1.1434, Test loss: 1.2348\n",
      "Epoch 27, Iteration 82500, Train loss: 1.1373, Test loss: 1.2375\n",
      "Epoch 27, Iteration 82600, Train loss: 1.1391, Test loss: 1.2405\n",
      "Epoch 27, Iteration 82700, Train loss: 1.1379, Test loss: 1.2327\n",
      "Epoch 27, Iteration 82800, Train loss: 1.1385, Test loss: 1.2325\n",
      "Epoch 27, Iteration 82900, Train loss: 1.1399, Test loss: 1.2339\n",
      "Epoch 27, Iteration 83000, Train loss: 1.1405, Test loss: 1.2325\n",
      "Epoch 27, Iteration 83100, Train loss: 1.1359, Test loss: 1.2355\n",
      "Epoch 27, Iteration 83200, Train loss: 1.1398, Test loss: 1.2386\n",
      "Epoch 27, Iteration 83300, Train loss: 1.1414, Test loss: 1.2365\n",
      "Epoch 27, Iteration 83400, Train loss: 1.1362, Test loss: 1.2386\n",
      "Epoch 27, Iteration 83500, Train loss: 1.1407, Test loss: 1.2330\n",
      "Epoch 27, Iteration 83600, Train loss: 1.1335, Test loss: 1.2334\n",
      "Epoch 27, Iteration 83700, Train loss: 1.1444, Test loss: 1.2344\n",
      "Epoch 27, Iteration 83800, Train loss: 1.1393, Test loss: 1.2352\n",
      "Epoch 27, Iteration 83900, Train loss: 1.1344, Test loss: 1.2335\n",
      "Epoch 27: Model saved\n",
      "Epoch 28: Model loaded\n",
      "Epoch 28, Iteration 84000, Train loss: 1.1391, Test loss: 1.2343\n",
      "Epoch 28, Iteration 84100, Train loss: 1.1368, Test loss: 1.2284\n",
      "Epoch 28, Iteration 84200, Train loss: 1.1341, Test loss: 1.2316\n",
      "Epoch 28, Iteration 84300, Train loss: 1.1415, Test loss: 1.2339\n",
      "Epoch 28, Iteration 84400, Train loss: 1.1389, Test loss: 1.2347\n",
      "Epoch 28, Iteration 84500, Train loss: 1.1343, Test loss: 1.2348\n",
      "Epoch 28, Iteration 84600, Train loss: 1.1372, Test loss: 1.2314\n",
      "Epoch 28, Iteration 84700, Train loss: 1.1377, Test loss: 1.2307\n",
      "Epoch 28, Iteration 84800, Train loss: 1.1368, Test loss: 1.2361\n",
      "Epoch 28, Iteration 84900, Train loss: 1.1360, Test loss: 1.2307\n",
      "Epoch 28, Iteration 85000, Train loss: 1.1371, Test loss: 1.2310\n",
      "Epoch 28, Iteration 85100, Train loss: 1.1378, Test loss: 1.2312\n",
      "Epoch 28, Iteration 85200, Train loss: 1.1354, Test loss: 1.2328\n",
      "Epoch 28, Iteration 85300, Train loss: 1.1348, Test loss: 1.2347\n",
      "Epoch 28, Iteration 85400, Train loss: 1.1366, Test loss: 1.2334\n",
      "Epoch 28, Iteration 85500, Train loss: 1.1339, Test loss: 1.2319\n",
      "Epoch 28, Iteration 85600, Train loss: 1.1370, Test loss: 1.2380\n",
      "Epoch 28, Iteration 85700, Train loss: 1.1352, Test loss: 1.2369\n",
      "Epoch 28, Iteration 85800, Train loss: 1.1318, Test loss: 1.2316\n",
      "Epoch 28, Iteration 85900, Train loss: 1.1372, Test loss: 1.2336\n",
      "Epoch 28, Iteration 86000, Train loss: 1.1357, Test loss: 1.2338\n",
      "Epoch 28, Iteration 86100, Train loss: 1.1342, Test loss: 1.2312\n",
      "Epoch 28, Iteration 86200, Train loss: 1.1344, Test loss: 1.2293\n",
      "Epoch 28, Iteration 86300, Train loss: 1.1335, Test loss: 1.2340\n",
      "Epoch 28, Iteration 86400, Train loss: 1.1309, Test loss: 1.2313\n",
      "Epoch 28, Iteration 86500, Train loss: 1.1365, Test loss: 1.2325\n",
      "Epoch 28, Iteration 86600, Train loss: 1.1358, Test loss: 1.2325\n",
      "Epoch 28, Iteration 86700, Train loss: 1.1322, Test loss: 1.2305\n",
      "Epoch 28, Iteration 86800, Train loss: 1.1332, Test loss: 1.2328\n",
      "Epoch 28, Iteration 86900, Train loss: 1.1293, Test loss: 1.2320\n",
      "Epoch 28: Model saved\n",
      "Epoch 29: Model loaded\n",
      "Epoch 29, Iteration 87000, Train loss: 1.1329, Test loss: 1.2314\n",
      "Epoch 29, Iteration 87100, Train loss: 1.1305, Test loss: 1.2314\n",
      "Epoch 29, Iteration 87200, Train loss: 1.1351, Test loss: 1.2289\n",
      "Epoch 29, Iteration 87300, Train loss: 1.1340, Test loss: 1.2302\n",
      "Epoch 29, Iteration 87400, Train loss: 1.1316, Test loss: 1.2309\n",
      "Epoch 29, Iteration 87500, Train loss: 1.1353, Test loss: 1.2282\n",
      "Epoch 29, Iteration 87600, Train loss: 1.1326, Test loss: 1.2307\n",
      "Epoch 29, Iteration 87700, Train loss: 1.1305, Test loss: 1.2272\n",
      "Epoch 29, Iteration 87800, Train loss: 1.1324, Test loss: 1.2286\n",
      "Epoch 29, Iteration 87900, Train loss: 1.1370, Test loss: 1.2343\n",
      "Epoch 29, Iteration 88000, Train loss: 1.1339, Test loss: 1.2346\n",
      "Epoch 29, Iteration 88100, Train loss: 1.1286, Test loss: 1.2343\n",
      "Epoch 29, Iteration 88200, Train loss: 1.1323, Test loss: 1.2284\n",
      "Epoch 29, Iteration 88300, Train loss: 1.1295, Test loss: 1.2341\n",
      "Epoch 29, Iteration 88400, Train loss: 1.1312, Test loss: 1.2215\n",
      "Epoch 29, Iteration 88500, Train loss: 1.1328, Test loss: 1.2300\n",
      "Epoch 29, Iteration 88600, Train loss: 1.1279, Test loss: 1.2227\n",
      "Epoch 29, Iteration 88700, Train loss: 1.1286, Test loss: 1.2309\n",
      "Epoch 29, Iteration 88800, Train loss: 1.1300, Test loss: 1.2261\n",
      "Epoch 29, Iteration 88900, Train loss: 1.1297, Test loss: 1.2321\n",
      "Epoch 29, Iteration 89000, Train loss: 1.1244, Test loss: 1.2256\n",
      "Epoch 29, Iteration 89100, Train loss: 1.1324, Test loss: 1.2290\n",
      "Epoch 29, Iteration 89200, Train loss: 1.1270, Test loss: 1.2253\n",
      "Epoch 29, Iteration 89300, Train loss: 1.1267, Test loss: 1.2319\n",
      "Epoch 29, Iteration 89400, Train loss: 1.1291, Test loss: 1.2329\n",
      "Epoch 29, Iteration 89500, Train loss: 1.1259, Test loss: 1.2277\n",
      "Epoch 29, Iteration 89600, Train loss: 1.1308, Test loss: 1.2276\n",
      "Epoch 29, Iteration 89700, Train loss: 1.1315, Test loss: 1.2268\n",
      "Epoch 29, Iteration 89800, Train loss: 1.1274, Test loss: 1.2357\n",
      "Epoch 29, Iteration 89900, Train loss: 1.1322, Test loss: 1.2272\n",
      "Epoch 29: Model saved\n",
      "Epoch 30: Model loaded\n",
      "Epoch 30, Iteration 90000, Train loss: 1.1286, Test loss: 1.2350\n",
      "Epoch 30, Iteration 90100, Train loss: 1.1285, Test loss: 1.2305\n",
      "Epoch 30, Iteration 90200, Train loss: 1.1293, Test loss: 1.2256\n",
      "Epoch 30, Iteration 90300, Train loss: 1.1288, Test loss: 1.2234\n",
      "Epoch 30, Iteration 90400, Train loss: 1.1269, Test loss: 1.2208\n",
      "Epoch 30, Iteration 90500, Train loss: 1.1289, Test loss: 1.2272\n",
      "Epoch 30, Iteration 90600, Train loss: 1.1312, Test loss: 1.2307\n",
      "Epoch 30, Iteration 90700, Train loss: 1.1254, Test loss: 1.2278\n",
      "Epoch 30, Iteration 90800, Train loss: 1.1233, Test loss: 1.2282\n",
      "Epoch 30, Iteration 90900, Train loss: 1.1262, Test loss: 1.2327\n",
      "Epoch 30, Iteration 91000, Train loss: 1.1249, Test loss: 1.2240\n",
      "Epoch 30, Iteration 91100, Train loss: 1.1266, Test loss: 1.2293\n",
      "Epoch 30, Iteration 91200, Train loss: 1.1275, Test loss: 1.2285\n",
      "Epoch 30, Iteration 91300, Train loss: 1.1229, Test loss: 1.2266\n",
      "Epoch 30, Iteration 91400, Train loss: 1.1276, Test loss: 1.2284\n",
      "Epoch 30, Iteration 91500, Train loss: 1.1167, Test loss: 1.2269\n",
      "Epoch 30, Iteration 91600, Train loss: 1.1276, Test loss: 1.2286\n",
      "Epoch 30, Iteration 91700, Train loss: 1.1278, Test loss: 1.2242\n",
      "Epoch 30, Iteration 91800, Train loss: 1.1256, Test loss: 1.2236\n",
      "Epoch 30, Iteration 91900, Train loss: 1.1234, Test loss: 1.2227\n",
      "Epoch 30, Iteration 92000, Train loss: 1.1220, Test loss: 1.2296\n",
      "Epoch 30, Iteration 92100, Train loss: 1.1284, Test loss: 1.2282\n",
      "Epoch 30, Iteration 92200, Train loss: 1.1269, Test loss: 1.2275\n",
      "Epoch 30, Iteration 92300, Train loss: 1.1205, Test loss: 1.2299\n",
      "Epoch 30, Iteration 92400, Train loss: 1.1249, Test loss: 1.2295\n",
      "Epoch 30, Iteration 92500, Train loss: 1.1261, Test loss: 1.2237\n",
      "Epoch 30, Iteration 92600, Train loss: 1.1150, Test loss: 1.2308\n",
      "Epoch 30, Iteration 92700, Train loss: 1.1242, Test loss: 1.2258\n",
      "Epoch 30, Iteration 92800, Train loss: 1.1252, Test loss: 1.2271\n",
      "Epoch 30, Iteration 92900, Train loss: 1.1261, Test loss: 1.2245\n",
      "Epoch 30: Model saved\n",
      "Epoch 31: Model loaded\n",
      "Epoch 31, Iteration 93000, Train loss: 1.1225, Test loss: 1.2256\n",
      "Epoch 31, Iteration 93100, Train loss: 1.1238, Test loss: 1.2256\n",
      "Epoch 31, Iteration 93200, Train loss: 1.1255, Test loss: 1.2219\n",
      "Epoch 31, Iteration 93300, Train loss: 1.1209, Test loss: 1.2241\n",
      "Epoch 31, Iteration 93400, Train loss: 1.1187, Test loss: 1.2223\n",
      "Epoch 31, Iteration 93500, Train loss: 1.1240, Test loss: 1.2297\n",
      "Epoch 31, Iteration 93600, Train loss: 1.1188, Test loss: 1.2247\n",
      "Epoch 31, Iteration 93700, Train loss: 1.1204, Test loss: 1.2234\n",
      "Epoch 31, Iteration 93800, Train loss: 1.1212, Test loss: 1.2262\n",
      "Epoch 31, Iteration 93900, Train loss: 1.1220, Test loss: 1.2254\n",
      "Epoch 31, Iteration 94000, Train loss: 1.1208, Test loss: 1.2256\n",
      "Epoch 31, Iteration 94100, Train loss: 1.1207, Test loss: 1.2257\n",
      "Epoch 31, Iteration 94200, Train loss: 1.1227, Test loss: 1.2262\n",
      "Epoch 31, Iteration 94300, Train loss: 1.1199, Test loss: 1.2272\n",
      "Epoch 31, Iteration 94400, Train loss: 1.1232, Test loss: 1.2239\n",
      "Epoch 31, Iteration 94500, Train loss: 1.1245, Test loss: 1.2261\n",
      "Epoch 31, Iteration 94600, Train loss: 1.1213, Test loss: 1.2230\n",
      "Epoch 31, Iteration 94700, Train loss: 1.1169, Test loss: 1.2209\n",
      "Epoch 31, Iteration 94800, Train loss: 1.1211, Test loss: 1.2272\n",
      "Epoch 31, Iteration 94900, Train loss: 1.1204, Test loss: 1.2293\n",
      "Epoch 31, Iteration 95000, Train loss: 1.1173, Test loss: 1.2250\n",
      "Epoch 31, Iteration 95100, Train loss: 1.1175, Test loss: 1.2247\n",
      "Epoch 31, Iteration 95200, Train loss: 1.1180, Test loss: 1.2195\n",
      "Epoch 31, Iteration 95300, Train loss: 1.1197, Test loss: 1.2322\n",
      "Epoch 31, Iteration 95400, Train loss: 1.1177, Test loss: 1.2251\n",
      "Epoch 31, Iteration 95500, Train loss: 1.1200, Test loss: 1.2294\n",
      "Epoch 31, Iteration 95600, Train loss: 1.1184, Test loss: 1.2305\n",
      "Epoch 31, Iteration 95700, Train loss: 1.1226, Test loss: 1.2248\n",
      "Epoch 31, Iteration 95800, Train loss: 1.1206, Test loss: 1.2202\n",
      "Epoch 31, Iteration 95900, Train loss: 1.1207, Test loss: 1.2243\n",
      "Epoch 31: Model saved\n",
      "Epoch 32: Model loaded\n",
      "Epoch 32, Iteration 96000, Train loss: 1.1161, Test loss: 1.2240\n",
      "Epoch 32, Iteration 96100, Train loss: 1.1190, Test loss: 1.2205\n",
      "Epoch 32, Iteration 96200, Train loss: 1.1174, Test loss: 1.2249\n",
      "Epoch 32, Iteration 96300, Train loss: 1.1173, Test loss: 1.2236\n",
      "Epoch 32, Iteration 96400, Train loss: 1.1177, Test loss: 1.2184\n",
      "Epoch 32, Iteration 96500, Train loss: 1.1210, Test loss: 1.2187\n",
      "Epoch 32, Iteration 96600, Train loss: 1.1196, Test loss: 1.2199\n",
      "Epoch 32, Iteration 96700, Train loss: 1.1210, Test loss: 1.2259\n",
      "Epoch 32, Iteration 96800, Train loss: 1.1175, Test loss: 1.2231\n",
      "Epoch 32, Iteration 96900, Train loss: 1.1187, Test loss: 1.2207\n",
      "Epoch 32, Iteration 97000, Train loss: 1.1185, Test loss: 1.2220\n",
      "Epoch 32, Iteration 97100, Train loss: 1.1142, Test loss: 1.2226\n",
      "Epoch 32, Iteration 97200, Train loss: 1.1205, Test loss: 1.2234\n",
      "Epoch 32, Iteration 97300, Train loss: 1.1174, Test loss: 1.2192\n",
      "Epoch 32, Iteration 97400, Train loss: 1.1138, Test loss: 1.2251\n",
      "Epoch 32, Iteration 97500, Train loss: 1.1168, Test loss: 1.2190\n",
      "Epoch 32, Iteration 97600, Train loss: 1.1152, Test loss: 1.2210\n",
      "Epoch 32, Iteration 97700, Train loss: 1.1155, Test loss: 1.2275\n",
      "Epoch 32, Iteration 97800, Train loss: 1.1151, Test loss: 1.2310\n",
      "Epoch 32, Iteration 97900, Train loss: 1.1145, Test loss: 1.2210\n",
      "Epoch 32, Iteration 98000, Train loss: 1.1156, Test loss: 1.2204\n",
      "Epoch 32, Iteration 98100, Train loss: 1.1135, Test loss: 1.2219\n",
      "Epoch 32, Iteration 98200, Train loss: 1.1161, Test loss: 1.2193\n",
      "Epoch 32, Iteration 98300, Train loss: 1.1177, Test loss: 1.2196\n",
      "Epoch 32, Iteration 98400, Train loss: 1.1125, Test loss: 1.2218\n",
      "Epoch 32, Iteration 98500, Train loss: 1.1145, Test loss: 1.2241\n",
      "Epoch 32, Iteration 98600, Train loss: 1.1127, Test loss: 1.2220\n",
      "Epoch 32, Iteration 98700, Train loss: 1.1133, Test loss: 1.2216\n",
      "Epoch 32, Iteration 98800, Train loss: 1.1120, Test loss: 1.2195\n",
      "Epoch 32, Iteration 98900, Train loss: 1.1130, Test loss: 1.2256\n",
      "Epoch 32: Model saved\n",
      "Epoch 33: Model loaded\n",
      "Epoch 33, Iteration 99000, Train loss: 1.1154, Test loss: 1.2202\n",
      "Epoch 33, Iteration 99100, Train loss: 1.1180, Test loss: 1.2235\n",
      "Epoch 33, Iteration 99200, Train loss: 1.1154, Test loss: 1.2242\n",
      "Epoch 33, Iteration 99300, Train loss: 1.1151, Test loss: 1.2188\n",
      "Epoch 33, Iteration 99400, Train loss: 1.1124, Test loss: 1.2212\n",
      "Epoch 33, Iteration 99500, Train loss: 1.1142, Test loss: 1.2199\n",
      "Epoch 33, Iteration 99600, Train loss: 1.1176, Test loss: 1.2240\n",
      "Epoch 33, Iteration 99700, Train loss: 1.1099, Test loss: 1.2182\n",
      "Epoch 33, Iteration 99800, Train loss: 1.1132, Test loss: 1.2181\n",
      "Epoch 33, Iteration 99900, Train loss: 1.1178, Test loss: 1.2237\n",
      "Epoch 33, Iteration 100000, Train loss: 1.1086, Test loss: 1.2204\n",
      "Epoch 33, Iteration 100100, Train loss: 1.1095, Test loss: 1.2195\n",
      "Epoch 33, Iteration 100200, Train loss: 1.1152, Test loss: 1.2236\n",
      "Epoch 33, Iteration 100300, Train loss: 1.1089, Test loss: 1.2168\n",
      "Epoch 33, Iteration 100400, Train loss: 1.1125, Test loss: 1.2196\n",
      "Epoch 33, Iteration 100500, Train loss: 1.1132, Test loss: 1.2172\n",
      "Epoch 33, Iteration 100600, Train loss: 1.1097, Test loss: 1.2176\n",
      "Epoch 33, Iteration 100700, Train loss: 1.1114, Test loss: 1.2213\n",
      "Epoch 33, Iteration 100800, Train loss: 1.1134, Test loss: 1.2165\n",
      "Epoch 33, Iteration 100900, Train loss: 1.1141, Test loss: 1.2254\n",
      "Epoch 33, Iteration 101000, Train loss: 1.1108, Test loss: 1.2220\n",
      "Epoch 33, Iteration 101100, Train loss: 1.1109, Test loss: 1.2173\n",
      "Epoch 33, Iteration 101200, Train loss: 1.1126, Test loss: 1.2226\n",
      "Epoch 33, Iteration 101300, Train loss: 1.1121, Test loss: 1.2227\n",
      "Epoch 33, Iteration 101400, Train loss: 1.1140, Test loss: 1.2263\n",
      "Epoch 33, Iteration 101500, Train loss: 1.1116, Test loss: 1.2209\n",
      "Epoch 33, Iteration 101600, Train loss: 1.1097, Test loss: 1.2121\n",
      "Epoch 33, Iteration 101700, Train loss: 1.1121, Test loss: 1.2229\n",
      "Epoch 33, Iteration 101800, Train loss: 1.1113, Test loss: 1.2199\n",
      "Epoch 33, Iteration 101900, Train loss: 1.1114, Test loss: 1.2217\n",
      "Epoch 33: Model saved\n",
      "Epoch 34: Model loaded\n",
      "Epoch 34, Iteration 102000, Train loss: 1.1079, Test loss: 1.2199\n",
      "Epoch 34, Iteration 102100, Train loss: 1.1120, Test loss: 1.2215\n",
      "Epoch 34, Iteration 102200, Train loss: 1.1123, Test loss: 1.2182\n",
      "Epoch 34, Iteration 102300, Train loss: 1.1063, Test loss: 1.2191\n",
      "Epoch 34, Iteration 102400, Train loss: 1.1043, Test loss: 1.2241\n",
      "Epoch 34, Iteration 102500, Train loss: 1.1050, Test loss: 1.2187\n",
      "Epoch 34, Iteration 102600, Train loss: 1.1065, Test loss: 1.2202\n",
      "Epoch 34, Iteration 102700, Train loss: 1.1083, Test loss: 1.2176\n",
      "Epoch 34, Iteration 102800, Train loss: 1.1081, Test loss: 1.2177\n",
      "Epoch 34, Iteration 102900, Train loss: 1.1088, Test loss: 1.2213\n",
      "Epoch 34, Iteration 103000, Train loss: 1.1095, Test loss: 1.2154\n",
      "Epoch 34, Iteration 103100, Train loss: 1.1023, Test loss: 1.2070\n",
      "Epoch 34, Iteration 103200, Train loss: 1.1067, Test loss: 1.2162\n",
      "Epoch 34, Iteration 103300, Train loss: 1.1077, Test loss: 1.2166\n",
      "Epoch 34, Iteration 103400, Train loss: 1.1046, Test loss: 1.2163\n",
      "Epoch 34, Iteration 103500, Train loss: 1.1074, Test loss: 1.2172\n",
      "Epoch 34, Iteration 103600, Train loss: 1.1070, Test loss: 1.2143\n",
      "Epoch 34, Iteration 103700, Train loss: 1.1093, Test loss: 1.2192\n",
      "Epoch 34, Iteration 103800, Train loss: 1.1044, Test loss: 1.2130\n",
      "Epoch 34, Iteration 103900, Train loss: 1.1087, Test loss: 1.2136\n",
      "Epoch 34, Iteration 104000, Train loss: 1.1082, Test loss: 1.2168\n",
      "Epoch 34, Iteration 104100, Train loss: 1.1037, Test loss: 1.2157\n",
      "Epoch 34, Iteration 104200, Train loss: 1.1051, Test loss: 1.2178\n",
      "Epoch 34, Iteration 104300, Train loss: 1.1078, Test loss: 1.2193\n",
      "Epoch 34, Iteration 104400, Train loss: 1.1107, Test loss: 1.2214\n",
      "Epoch 34, Iteration 104500, Train loss: 1.1091, Test loss: 1.2136\n",
      "Epoch 34, Iteration 104600, Train loss: 1.1050, Test loss: 1.2175\n",
      "Epoch 34, Iteration 104700, Train loss: 1.1047, Test loss: 1.2203\n",
      "Epoch 34, Iteration 104800, Train loss: 1.1061, Test loss: 1.2158\n",
      "Epoch 34, Iteration 104900, Train loss: 1.1038, Test loss: 1.2171\n",
      "Epoch 34: Model saved\n",
      "Epoch 35: Model loaded\n",
      "Epoch 35, Iteration 105000, Train loss: 1.1051, Test loss: 1.2142\n",
      "Epoch 35, Iteration 105100, Train loss: 1.1025, Test loss: 1.2158\n",
      "Epoch 35, Iteration 105200, Train loss: 1.1046, Test loss: 1.2137\n",
      "Epoch 35, Iteration 105300, Train loss: 1.1058, Test loss: 1.2143\n",
      "Epoch 35, Iteration 105400, Train loss: 1.1105, Test loss: 1.2145\n",
      "Epoch 35, Iteration 105500, Train loss: 1.1028, Test loss: 1.2141\n",
      "Epoch 35, Iteration 105600, Train loss: 1.1019, Test loss: 1.2127\n",
      "Epoch 35, Iteration 105700, Train loss: 1.1016, Test loss: 1.2154\n",
      "Epoch 35, Iteration 105800, Train loss: 1.1026, Test loss: 1.2142\n",
      "Epoch 35, Iteration 105900, Train loss: 1.1053, Test loss: 1.2160\n",
      "Epoch 35, Iteration 106000, Train loss: 1.1019, Test loss: 1.2179\n",
      "Epoch 35, Iteration 106100, Train loss: 1.1025, Test loss: 1.2186\n",
      "Epoch 35, Iteration 106200, Train loss: 1.1035, Test loss: 1.2140\n",
      "Epoch 35, Iteration 106300, Train loss: 1.1033, Test loss: 1.2147\n",
      "Epoch 35, Iteration 106400, Train loss: 1.1011, Test loss: 1.2190\n",
      "Epoch 35, Iteration 106500, Train loss: 1.0977, Test loss: 1.2181\n",
      "Epoch 35, Iteration 106600, Train loss: 1.1022, Test loss: 1.2188\n",
      "Epoch 35, Iteration 106700, Train loss: 1.1041, Test loss: 1.2149\n",
      "Epoch 35, Iteration 106800, Train loss: 1.1026, Test loss: 1.2137\n",
      "Epoch 35, Iteration 106900, Train loss: 1.1029, Test loss: 1.2230\n",
      "Epoch 35, Iteration 107000, Train loss: 1.0973, Test loss: 1.2167\n",
      "Epoch 35, Iteration 107100, Train loss: 1.1073, Test loss: 1.2172\n",
      "Epoch 35, Iteration 107200, Train loss: 1.0987, Test loss: 1.2168\n",
      "Epoch 35, Iteration 107300, Train loss: 1.1039, Test loss: 1.2216\n",
      "Epoch 35, Iteration 107400, Train loss: 1.1025, Test loss: 1.2114\n",
      "Epoch 35, Iteration 107500, Train loss: 1.1027, Test loss: 1.2166\n",
      "Epoch 35, Iteration 107600, Train loss: 1.1010, Test loss: 1.2147\n",
      "Epoch 35, Iteration 107700, Train loss: 1.1021, Test loss: 1.2126\n",
      "Epoch 35, Iteration 107800, Train loss: 1.1006, Test loss: 1.2153\n",
      "Epoch 35, Iteration 107900, Train loss: 1.1023, Test loss: 1.2223\n",
      "Epoch 35: Model saved\n",
      "Epoch 36: Model loaded\n",
      "Epoch 36, Iteration 108000, Train loss: 1.0996, Test loss: 1.2151\n",
      "Epoch 36, Iteration 108100, Train loss: 1.1033, Test loss: 1.2156\n",
      "Epoch 36, Iteration 108200, Train loss: 1.1014, Test loss: 1.2137\n",
      "Epoch 36, Iteration 108300, Train loss: 1.0976, Test loss: 1.2090\n",
      "Epoch 36, Iteration 108400, Train loss: 1.1028, Test loss: 1.2158\n",
      "Epoch 36, Iteration 108500, Train loss: 1.0977, Test loss: 1.2128\n",
      "Epoch 36, Iteration 108600, Train loss: 1.1022, Test loss: 1.2102\n",
      "Epoch 36, Iteration 108700, Train loss: 1.0981, Test loss: 1.2121\n",
      "Epoch 36, Iteration 108800, Train loss: 1.1015, Test loss: 1.2157\n",
      "Epoch 36, Iteration 108900, Train loss: 1.1026, Test loss: 1.2126\n",
      "Epoch 36, Iteration 109000, Train loss: 1.1023, Test loss: 1.2163\n",
      "Epoch 36, Iteration 109100, Train loss: 1.0988, Test loss: 1.2159\n",
      "Epoch 36, Iteration 109200, Train loss: 1.0969, Test loss: 1.2153\n",
      "Epoch 36, Iteration 109300, Train loss: 1.1009, Test loss: 1.2123\n",
      "Epoch 36, Iteration 109400, Train loss: 1.0977, Test loss: 1.2061\n",
      "Epoch 36, Iteration 109500, Train loss: 1.1000, Test loss: 1.2143\n",
      "Epoch 36, Iteration 109600, Train loss: 1.0985, Test loss: 1.2142\n",
      "Epoch 36, Iteration 109700, Train loss: 1.0987, Test loss: 1.2138\n",
      "Epoch 36, Iteration 109800, Train loss: 1.0974, Test loss: 1.2188\n",
      "Epoch 36, Iteration 109900, Train loss: 1.1002, Test loss: 1.2135\n",
      "Epoch 36, Iteration 110000, Train loss: 1.0950, Test loss: 1.2160\n",
      "Epoch 36, Iteration 110100, Train loss: 1.1039, Test loss: 1.2133\n",
      "Epoch 36, Iteration 110200, Train loss: 1.0951, Test loss: 1.2187\n",
      "Epoch 36, Iteration 110300, Train loss: 1.0967, Test loss: 1.2129\n",
      "Epoch 36, Iteration 110400, Train loss: 1.1008, Test loss: 1.2122\n",
      "Epoch 36, Iteration 110500, Train loss: 1.0992, Test loss: 1.2097\n",
      "Epoch 36, Iteration 110600, Train loss: 1.1019, Test loss: 1.2125\n",
      "Epoch 36, Iteration 110700, Train loss: 1.0981, Test loss: 1.2103\n",
      "Epoch 36, Iteration 110800, Train loss: 1.1010, Test loss: 1.2079\n",
      "Epoch 36, Iteration 110900, Train loss: 1.0995, Test loss: 1.2142\n",
      "Epoch 36: Model saved\n",
      "Epoch 37: Model loaded\n",
      "Epoch 37, Iteration 111000, Train loss: 1.1002, Test loss: 1.2108\n",
      "Epoch 37, Iteration 111100, Train loss: 1.0957, Test loss: 1.2157\n",
      "Epoch 37, Iteration 111200, Train loss: 1.0971, Test loss: 1.2082\n",
      "Epoch 37, Iteration 111300, Train loss: 1.0973, Test loss: 1.2118\n",
      "Epoch 37, Iteration 111400, Train loss: 1.0953, Test loss: 1.2150\n",
      "Epoch 37, Iteration 111500, Train loss: 1.0977, Test loss: 1.2137\n",
      "Epoch 37, Iteration 111600, Train loss: 1.0964, Test loss: 1.2135\n",
      "Epoch 37, Iteration 111700, Train loss: 1.0965, Test loss: 1.2089\n",
      "Epoch 37, Iteration 111800, Train loss: 1.0928, Test loss: 1.2118\n",
      "Epoch 37, Iteration 111900, Train loss: 1.0983, Test loss: 1.2136\n",
      "Epoch 37, Iteration 112000, Train loss: 1.0965, Test loss: 1.2144\n",
      "Epoch 37, Iteration 112100, Train loss: 1.0949, Test loss: 1.2090\n",
      "Epoch 37, Iteration 112200, Train loss: 1.0952, Test loss: 1.2165\n",
      "Epoch 37, Iteration 112300, Train loss: 1.0956, Test loss: 1.2122\n",
      "Epoch 37, Iteration 112400, Train loss: 1.0927, Test loss: 1.2184\n",
      "Epoch 37, Iteration 112500, Train loss: 1.0965, Test loss: 1.2157\n",
      "Epoch 37, Iteration 112600, Train loss: 1.0925, Test loss: 1.2127\n",
      "Epoch 37, Iteration 112700, Train loss: 1.0898, Test loss: 1.2078\n",
      "Epoch 37, Iteration 112800, Train loss: 1.0937, Test loss: 1.2082\n",
      "Epoch 37, Iteration 112900, Train loss: 1.0957, Test loss: 1.2132\n",
      "Epoch 37, Iteration 113000, Train loss: 1.0911, Test loss: 1.2102\n",
      "Epoch 37, Iteration 113100, Train loss: 1.0957, Test loss: 1.2157\n",
      "Epoch 37, Iteration 113200, Train loss: 1.0895, Test loss: 1.2113\n",
      "Epoch 37, Iteration 113300, Train loss: 1.0952, Test loss: 1.2123\n",
      "Epoch 37, Iteration 113400, Train loss: 1.0923, Test loss: 1.2105\n",
      "Epoch 37, Iteration 113500, Train loss: 1.0905, Test loss: 1.2104\n",
      "Epoch 37, Iteration 113600, Train loss: 1.0968, Test loss: 1.2131\n",
      "Epoch 37, Iteration 113700, Train loss: 1.0941, Test loss: 1.2132\n",
      "Epoch 37, Iteration 113800, Train loss: 1.0927, Test loss: 1.2136\n",
      "Epoch 37, Iteration 113900, Train loss: 1.0895, Test loss: 1.2108\n",
      "Epoch 37: Model saved\n",
      "Epoch 38: Model loaded\n",
      "Epoch 38, Iteration 114000, Train loss: 1.0918, Test loss: 1.2132\n",
      "Epoch 38, Iteration 114100, Train loss: 1.0921, Test loss: 1.2046\n",
      "Epoch 38, Iteration 114200, Train loss: 1.0966, Test loss: 1.2107\n",
      "Epoch 38, Iteration 114300, Train loss: 1.0922, Test loss: 1.2146\n",
      "Epoch 38, Iteration 114400, Train loss: 1.0922, Test loss: 1.2089\n",
      "Epoch 38, Iteration 114500, Train loss: 1.0868, Test loss: 1.2084\n",
      "Epoch 38, Iteration 114600, Train loss: 1.0910, Test loss: 1.2052\n",
      "Epoch 38, Iteration 114700, Train loss: 1.0930, Test loss: 1.2098\n",
      "Epoch 38, Iteration 114800, Train loss: 1.0886, Test loss: 1.2075\n",
      "Epoch 38, Iteration 114900, Train loss: 1.0895, Test loss: 1.2074\n",
      "Epoch 38, Iteration 115000, Train loss: 1.0907, Test loss: 1.2071\n",
      "Epoch 38, Iteration 115100, Train loss: 1.0931, Test loss: 1.2103\n",
      "Epoch 38, Iteration 115200, Train loss: 1.0925, Test loss: 1.2094\n",
      "Epoch 38, Iteration 115300, Train loss: 1.0913, Test loss: 1.2123\n",
      "Epoch 38, Iteration 115400, Train loss: 1.0913, Test loss: 1.2105\n",
      "Epoch 38, Iteration 115500, Train loss: 1.0916, Test loss: 1.2075\n",
      "Epoch 38, Iteration 115600, Train loss: 1.0912, Test loss: 1.2098\n",
      "Epoch 38, Iteration 115700, Train loss: 1.0890, Test loss: 1.2092\n",
      "Epoch 38, Iteration 115800, Train loss: 1.0916, Test loss: 1.2084\n",
      "Epoch 38, Iteration 115900, Train loss: 1.0930, Test loss: 1.2055\n",
      "Epoch 38, Iteration 116000, Train loss: 1.0883, Test loss: 1.2065\n",
      "Epoch 38, Iteration 116100, Train loss: 1.0948, Test loss: 1.2080\n",
      "Epoch 38, Iteration 116200, Train loss: 1.0885, Test loss: 1.2106\n",
      "Epoch 38, Iteration 116300, Train loss: 1.0907, Test loss: 1.2038\n",
      "Epoch 38, Iteration 116400, Train loss: 1.0894, Test loss: 1.2109\n",
      "Epoch 38, Iteration 116500, Train loss: 1.0884, Test loss: 1.2132\n",
      "Epoch 38, Iteration 116600, Train loss: 1.0915, Test loss: 1.2107\n",
      "Epoch 38, Iteration 116700, Train loss: 1.0909, Test loss: 1.2105\n",
      "Epoch 38, Iteration 116800, Train loss: 1.0887, Test loss: 1.2126\n",
      "Epoch 38, Iteration 116900, Train loss: 1.0893, Test loss: 1.2153\n",
      "Epoch 38: Model saved\n",
      "Epoch 39: Model loaded\n",
      "Epoch 39, Iteration 117000, Train loss: 1.0886, Test loss: 1.2115\n",
      "Epoch 39, Iteration 117100, Train loss: 1.0902, Test loss: 1.2102\n",
      "Epoch 39, Iteration 117200, Train loss: 1.0915, Test loss: 1.2092\n",
      "Epoch 39, Iteration 117300, Train loss: 1.0884, Test loss: 1.2085\n",
      "Epoch 39, Iteration 117400, Train loss: 1.0908, Test loss: 1.2097\n",
      "Epoch 39, Iteration 117500, Train loss: 1.0865, Test loss: 1.2099\n",
      "Epoch 39, Iteration 117600, Train loss: 1.0846, Test loss: 1.2068\n",
      "Epoch 39, Iteration 117700, Train loss: 1.0873, Test loss: 1.2089\n",
      "Epoch 39, Iteration 117800, Train loss: 1.0855, Test loss: 1.2102\n",
      "Epoch 39, Iteration 117900, Train loss: 1.0878, Test loss: 1.2069\n",
      "Epoch 39, Iteration 118000, Train loss: 1.0942, Test loss: 1.2114\n",
      "Epoch 39, Iteration 118100, Train loss: 1.0884, Test loss: 1.2125\n",
      "Epoch 39, Iteration 118200, Train loss: 1.0882, Test loss: 1.2046\n",
      "Epoch 39, Iteration 118300, Train loss: 1.0861, Test loss: 1.2119\n",
      "Epoch 39, Iteration 118400, Train loss: 1.0851, Test loss: 1.2055\n",
      "Epoch 39, Iteration 118500, Train loss: 1.0820, Test loss: 1.2092\n",
      "Epoch 39, Iteration 118600, Train loss: 1.0831, Test loss: 1.2043\n",
      "Epoch 39, Iteration 118700, Train loss: 1.0873, Test loss: 1.2077\n",
      "Epoch 39, Iteration 118800, Train loss: 1.0856, Test loss: 1.2129\n",
      "Epoch 39, Iteration 118900, Train loss: 1.0868, Test loss: 1.2048\n",
      "Epoch 39, Iteration 119000, Train loss: 1.0841, Test loss: 1.2054\n",
      "Epoch 39, Iteration 119100, Train loss: 1.0880, Test loss: 1.2107\n",
      "Epoch 39, Iteration 119200, Train loss: 1.0838, Test loss: 1.2114\n",
      "Epoch 39, Iteration 119300, Train loss: 1.0882, Test loss: 1.2113\n",
      "Epoch 39, Iteration 119400, Train loss: 1.0860, Test loss: 1.2088\n",
      "Epoch 39, Iteration 119500, Train loss: 1.0860, Test loss: 1.2082\n",
      "Epoch 39, Iteration 119600, Train loss: 1.0835, Test loss: 1.2097\n",
      "Epoch 39, Iteration 119700, Train loss: 1.0895, Test loss: 1.2030\n",
      "Epoch 39, Iteration 119800, Train loss: 1.0837, Test loss: 1.2048\n",
      "Epoch 39, Iteration 119900, Train loss: 1.0852, Test loss: 1.2053\n",
      "Epoch 39: Model saved\n",
      "Epoch 40: Model loaded\n",
      "Epoch 40, Iteration 120000, Train loss: 1.0873, Test loss: 1.2060\n",
      "Epoch 40, Iteration 120100, Train loss: 1.0837, Test loss: 1.2084\n",
      "Epoch 40, Iteration 120200, Train loss: 1.0833, Test loss: 1.2098\n",
      "Epoch 40, Iteration 120300, Train loss: 1.0886, Test loss: 1.2045\n",
      "Epoch 40, Iteration 120400, Train loss: 1.0821, Test loss: 1.1999\n",
      "Epoch 40, Iteration 120500, Train loss: 1.0860, Test loss: 1.2077\n",
      "Epoch 40, Iteration 120600, Train loss: 1.0845, Test loss: 1.2115\n",
      "Epoch 40, Iteration 120700, Train loss: 1.0843, Test loss: 1.2081\n",
      "Epoch 40, Iteration 120800, Train loss: 1.0847, Test loss: 1.2089\n",
      "Epoch 40, Iteration 120900, Train loss: 1.0842, Test loss: 1.2095\n",
      "Epoch 40, Iteration 121000, Train loss: 1.0821, Test loss: 1.2055\n",
      "Epoch 40, Iteration 121100, Train loss: 1.0812, Test loss: 1.2134\n",
      "Epoch 40, Iteration 121200, Train loss: 1.0818, Test loss: 1.2111\n",
      "Epoch 40, Iteration 121300, Train loss: 1.0821, Test loss: 1.2082\n",
      "Epoch 40, Iteration 121400, Train loss: 1.0838, Test loss: 1.2089\n",
      "Epoch 40, Iteration 121500, Train loss: 1.0842, Test loss: 1.2031\n",
      "Epoch 40, Iteration 121600, Train loss: 1.0855, Test loss: 1.2039\n",
      "Epoch 40, Iteration 121700, Train loss: 1.0839, Test loss: 1.2033\n",
      "Epoch 40, Iteration 121800, Train loss: 1.0807, Test loss: 1.2063\n",
      "Epoch 40, Iteration 121900, Train loss: 1.0798, Test loss: 1.2068\n",
      "Epoch 40, Iteration 122000, Train loss: 1.0827, Test loss: 1.2083\n",
      "Epoch 40, Iteration 122100, Train loss: 1.0811, Test loss: 1.2054\n",
      "Epoch 40, Iteration 122200, Train loss: 1.0820, Test loss: 1.2047\n",
      "Epoch 40, Iteration 122300, Train loss: 1.0836, Test loss: 1.2107\n",
      "Epoch 40, Iteration 122400, Train loss: 1.0795, Test loss: 1.2107\n",
      "Epoch 40, Iteration 122500, Train loss: 1.0801, Test loss: 1.2017\n",
      "Epoch 40, Iteration 122600, Train loss: 1.0793, Test loss: 1.2016\n",
      "Epoch 40, Iteration 122700, Train loss: 1.0823, Test loss: 1.2082\n",
      "Epoch 40, Iteration 122800, Train loss: 1.0822, Test loss: 1.2068\n",
      "Epoch 40, Iteration 122900, Train loss: 1.0810, Test loss: 1.2094\n",
      "Epoch 40: Model saved\n",
      "Epoch 41: Model loaded\n",
      "Epoch 41, Iteration 123000, Train loss: 1.0779, Test loss: 1.2089\n",
      "Epoch 41, Iteration 123100, Train loss: 1.0809, Test loss: 1.2041\n",
      "Epoch 41, Iteration 123200, Train loss: 1.0822, Test loss: 1.2045\n",
      "Epoch 41, Iteration 123300, Train loss: 1.0835, Test loss: 1.2030\n",
      "Epoch 41, Iteration 123400, Train loss: 1.0788, Test loss: 1.2120\n",
      "Epoch 41, Iteration 123500, Train loss: 1.0858, Test loss: 1.2048\n",
      "Epoch 41, Iteration 123600, Train loss: 1.0774, Test loss: 1.1994\n",
      "Epoch 41, Iteration 123700, Train loss: 1.0787, Test loss: 1.1978\n",
      "Epoch 41, Iteration 123800, Train loss: 1.0801, Test loss: 1.2021\n",
      "Epoch 41, Iteration 123900, Train loss: 1.0765, Test loss: 1.2064\n",
      "Epoch 41, Iteration 124000, Train loss: 1.0847, Test loss: 1.2084\n",
      "Epoch 41, Iteration 124100, Train loss: 1.0806, Test loss: 1.2027\n",
      "Epoch 41, Iteration 124200, Train loss: 1.0769, Test loss: 1.2076\n",
      "Epoch 41, Iteration 124300, Train loss: 1.0794, Test loss: 1.2090\n",
      "Epoch 41, Iteration 124400, Train loss: 1.0824, Test loss: 1.1990\n",
      "Epoch 41, Iteration 124500, Train loss: 1.0783, Test loss: 1.2047\n",
      "Epoch 41, Iteration 124600, Train loss: 1.0820, Test loss: 1.2065\n",
      "Epoch 41, Iteration 124700, Train loss: 1.0820, Test loss: 1.2082\n",
      "Epoch 41, Iteration 124800, Train loss: 1.0823, Test loss: 1.2064\n",
      "Epoch 41, Iteration 124900, Train loss: 1.0761, Test loss: 1.1991\n",
      "Epoch 41, Iteration 125000, Train loss: 1.0809, Test loss: 1.2017\n",
      "Epoch 41, Iteration 125100, Train loss: 1.0793, Test loss: 1.2071\n",
      "Epoch 41, Iteration 125200, Train loss: 1.0781, Test loss: 1.2010\n",
      "Epoch 41, Iteration 125300, Train loss: 1.0755, Test loss: 1.1964\n",
      "Epoch 41, Iteration 125400, Train loss: 1.0808, Test loss: 1.2050\n",
      "Epoch 41, Iteration 125500, Train loss: 1.0778, Test loss: 1.1979\n",
      "Epoch 41, Iteration 125600, Train loss: 1.0793, Test loss: 1.2055\n",
      "Epoch 41, Iteration 125700, Train loss: 1.0788, Test loss: 1.2079\n",
      "Epoch 41, Iteration 125800, Train loss: 1.0801, Test loss: 1.2062\n",
      "Epoch 41, Iteration 125900, Train loss: 1.0791, Test loss: 1.2055\n",
      "Epoch 41: Model saved\n",
      "Epoch 42: Model loaded\n",
      "Epoch 42, Iteration 126000, Train loss: 1.0756, Test loss: 1.2039\n",
      "Epoch 42, Iteration 126100, Train loss: 1.0750, Test loss: 1.2011\n",
      "Epoch 42, Iteration 126200, Train loss: 1.0785, Test loss: 1.2035\n",
      "Epoch 42, Iteration 126300, Train loss: 1.0752, Test loss: 1.2023\n",
      "Epoch 42, Iteration 126400, Train loss: 1.0762, Test loss: 1.2076\n",
      "Epoch 42, Iteration 126500, Train loss: 1.0777, Test loss: 1.2057\n",
      "Epoch 42, Iteration 126600, Train loss: 1.0800, Test loss: 1.2002\n",
      "Epoch 42, Iteration 126700, Train loss: 1.0770, Test loss: 1.2006\n",
      "Epoch 42, Iteration 126800, Train loss: 1.0819, Test loss: 1.2023\n",
      "Epoch 42, Iteration 126900, Train loss: 1.0759, Test loss: 1.2067\n",
      "Epoch 42, Iteration 127000, Train loss: 1.0746, Test loss: 1.2015\n",
      "Epoch 42, Iteration 127100, Train loss: 1.0742, Test loss: 1.2053\n",
      "Epoch 42, Iteration 127200, Train loss: 1.0786, Test loss: 1.2066\n",
      "Epoch 42, Iteration 127300, Train loss: 1.0786, Test loss: 1.2047\n",
      "Epoch 42, Iteration 127400, Train loss: 1.0775, Test loss: 1.1999\n",
      "Epoch 42, Iteration 127500, Train loss: 1.0736, Test loss: 1.2051\n",
      "Epoch 42, Iteration 127600, Train loss: 1.0722, Test loss: 1.2055\n",
      "Epoch 42, Iteration 127700, Train loss: 1.0747, Test loss: 1.2065\n",
      "Epoch 42, Iteration 127800, Train loss: 1.0758, Test loss: 1.2011\n",
      "Epoch 42, Iteration 127900, Train loss: 1.0753, Test loss: 1.2010\n",
      "Epoch 42, Iteration 128000, Train loss: 1.0745, Test loss: 1.1979\n",
      "Epoch 42, Iteration 128100, Train loss: 1.0745, Test loss: 1.2020\n",
      "Epoch 42, Iteration 128200, Train loss: 1.0764, Test loss: 1.2001\n",
      "Epoch 42, Iteration 128300, Train loss: 1.0768, Test loss: 1.2046\n",
      "Epoch 42, Iteration 128400, Train loss: 1.0718, Test loss: 1.1992\n",
      "Epoch 42, Iteration 128500, Train loss: 1.0758, Test loss: 1.2039\n",
      "Epoch 42, Iteration 128600, Train loss: 1.0746, Test loss: 1.2060\n",
      "Epoch 42, Iteration 128700, Train loss: 1.0715, Test loss: 1.2005\n",
      "Epoch 42, Iteration 128800, Train loss: 1.0763, Test loss: 1.2036\n",
      "Epoch 42, Iteration 128900, Train loss: 1.0748, Test loss: 1.1971\n",
      "Epoch 42: Model saved\n",
      "Epoch 43: Model loaded\n",
      "Epoch 43, Iteration 129000, Train loss: 1.0741, Test loss: 1.2024\n",
      "Epoch 43, Iteration 129100, Train loss: 1.0732, Test loss: 1.2046\n",
      "Epoch 43, Iteration 129200, Train loss: 1.0749, Test loss: 1.2071\n",
      "Epoch 43, Iteration 129300, Train loss: 1.0719, Test loss: 1.1985\n",
      "Epoch 43, Iteration 129400, Train loss: 1.0729, Test loss: 1.1964\n",
      "Epoch 43, Iteration 129500, Train loss: 1.0750, Test loss: 1.2051\n",
      "Epoch 43, Iteration 129600, Train loss: 1.0727, Test loss: 1.2027\n",
      "Epoch 43, Iteration 129700, Train loss: 1.0744, Test loss: 1.2104\n",
      "Epoch 43, Iteration 129800, Train loss: 1.0785, Test loss: 1.1983\n",
      "Epoch 43, Iteration 129900, Train loss: 1.0766, Test loss: 1.2061\n",
      "Epoch 43, Iteration 130000, Train loss: 1.0691, Test loss: 1.1968\n",
      "Epoch 43, Iteration 130100, Train loss: 1.0752, Test loss: 1.2000\n",
      "Epoch 43, Iteration 130200, Train loss: 1.0690, Test loss: 1.1987\n",
      "Epoch 43, Iteration 130300, Train loss: 1.0719, Test loss: 1.1994\n",
      "Epoch 43, Iteration 130400, Train loss: 1.0713, Test loss: 1.2044\n",
      "Epoch 43, Iteration 130500, Train loss: 1.0733, Test loss: 1.2039\n",
      "Epoch 43, Iteration 130600, Train loss: 1.0713, Test loss: 1.2024\n",
      "Epoch 43, Iteration 130700, Train loss: 1.0697, Test loss: 1.2022\n",
      "Epoch 43, Iteration 130800, Train loss: 1.0728, Test loss: 1.2053\n",
      "Epoch 43, Iteration 130900, Train loss: 1.0693, Test loss: 1.2028\n",
      "Epoch 43, Iteration 131000, Train loss: 1.0703, Test loss: 1.2022\n",
      "Epoch 43, Iteration 131100, Train loss: 1.0731, Test loss: 1.2020\n",
      "Epoch 43, Iteration 131200, Train loss: 1.0726, Test loss: 1.1990\n",
      "Epoch 43, Iteration 131300, Train loss: 1.0702, Test loss: 1.1980\n",
      "Epoch 43, Iteration 131400, Train loss: 1.0716, Test loss: 1.2041\n",
      "Epoch 43, Iteration 131500, Train loss: 1.0742, Test loss: 1.2028\n",
      "Epoch 43, Iteration 131600, Train loss: 1.0707, Test loss: 1.1962\n",
      "Epoch 43, Iteration 131700, Train loss: 1.0694, Test loss: 1.2013\n",
      "Epoch 43, Iteration 131800, Train loss: 1.0739, Test loss: 1.2048\n",
      "Epoch 43, Iteration 131900, Train loss: 1.0683, Test loss: 1.2005\n",
      "Epoch 43: Model saved\n",
      "Epoch 44: Model loaded\n",
      "Epoch 44, Iteration 132000, Train loss: 1.0749, Test loss: 1.2017\n",
      "Epoch 44, Iteration 132100, Train loss: 1.0716, Test loss: 1.2012\n",
      "Epoch 44, Iteration 132200, Train loss: 1.0686, Test loss: 1.2006\n",
      "Epoch 44, Iteration 132300, Train loss: 1.0738, Test loss: 1.2027\n",
      "Epoch 44, Iteration 132400, Train loss: 1.0705, Test loss: 1.1948\n",
      "Epoch 44, Iteration 132500, Train loss: 1.0694, Test loss: 1.2032\n",
      "Epoch 44, Iteration 132600, Train loss: 1.0703, Test loss: 1.2011\n",
      "Epoch 44, Iteration 132700, Train loss: 1.0684, Test loss: 1.2015\n",
      "Epoch 44, Iteration 132800, Train loss: 1.0714, Test loss: 1.2033\n",
      "Epoch 44, Iteration 132900, Train loss: 1.0702, Test loss: 1.1981\n",
      "Epoch 44, Iteration 133000, Train loss: 1.0676, Test loss: 1.2014\n",
      "Epoch 44, Iteration 133100, Train loss: 1.0737, Test loss: 1.1966\n",
      "Epoch 44, Iteration 133200, Train loss: 1.0677, Test loss: 1.1997\n",
      "Epoch 44, Iteration 133300, Train loss: 1.0720, Test loss: 1.1984\n",
      "Epoch 44, Iteration 133400, Train loss: 1.0684, Test loss: 1.1957\n",
      "Epoch 44, Iteration 133500, Train loss: 1.0695, Test loss: 1.2017\n",
      "Epoch 44, Iteration 133600, Train loss: 1.0714, Test loss: 1.2023\n",
      "Epoch 44, Iteration 133700, Train loss: 1.0695, Test loss: 1.1986\n",
      "Epoch 44, Iteration 133800, Train loss: 1.0691, Test loss: 1.2000\n",
      "Epoch 44, Iteration 133900, Train loss: 1.0668, Test loss: 1.2018\n",
      "Epoch 44, Iteration 134000, Train loss: 1.0701, Test loss: 1.2023\n",
      "Epoch 44, Iteration 134100, Train loss: 1.0716, Test loss: 1.2038\n",
      "Epoch 44, Iteration 134200, Train loss: 1.0657, Test loss: 1.1996\n",
      "Epoch 44, Iteration 134300, Train loss: 1.0715, Test loss: 1.1977\n",
      "Epoch 44, Iteration 134400, Train loss: 1.0687, Test loss: 1.2009\n",
      "Epoch 44, Iteration 134500, Train loss: 1.0733, Test loss: 1.2065\n",
      "Epoch 44, Iteration 134600, Train loss: 1.0689, Test loss: 1.2014\n",
      "Epoch 44, Iteration 134700, Train loss: 1.0649, Test loss: 1.2012\n",
      "Epoch 44, Iteration 134800, Train loss: 1.0681, Test loss: 1.2002\n",
      "Epoch 44, Iteration 134900, Train loss: 1.0705, Test loss: 1.1931\n",
      "Epoch 44: Model saved\n",
      "Epoch 45: Model loaded\n",
      "Epoch 45, Iteration 135000, Train loss: 1.0683, Test loss: 1.1993\n",
      "Epoch 45, Iteration 135100, Train loss: 1.0690, Test loss: 1.1972\n",
      "Epoch 45, Iteration 135200, Train loss: 1.0676, Test loss: 1.1947\n",
      "Epoch 45, Iteration 135300, Train loss: 1.0664, Test loss: 1.2003\n",
      "Epoch 45, Iteration 135400, Train loss: 1.0635, Test loss: 1.1975\n",
      "Epoch 45, Iteration 135500, Train loss: 1.0683, Test loss: 1.2054\n",
      "Epoch 45, Iteration 135600, Train loss: 1.0687, Test loss: 1.1996\n",
      "Epoch 45, Iteration 135700, Train loss: 1.0723, Test loss: 1.1992\n",
      "Epoch 45, Iteration 135800, Train loss: 1.0659, Test loss: 1.1990\n",
      "Epoch 45, Iteration 135900, Train loss: 1.0676, Test loss: 1.2023\n",
      "Epoch 45, Iteration 136000, Train loss: 1.0658, Test loss: 1.1998\n",
      "Epoch 45, Iteration 136100, Train loss: 1.0676, Test loss: 1.2019\n",
      "Epoch 45, Iteration 136200, Train loss: 1.0703, Test loss: 1.1997\n",
      "Epoch 45, Iteration 136300, Train loss: 1.0667, Test loss: 1.1960\n",
      "Epoch 45, Iteration 136400, Train loss: 1.0691, Test loss: 1.1993\n",
      "Epoch 45, Iteration 136500, Train loss: 1.0687, Test loss: 1.1978\n",
      "Epoch 45, Iteration 136600, Train loss: 1.0672, Test loss: 1.2012\n",
      "Epoch 45, Iteration 136700, Train loss: 1.0645, Test loss: 1.2000\n",
      "Epoch 45, Iteration 136800, Train loss: 1.0676, Test loss: 1.1955\n",
      "Epoch 45, Iteration 136900, Train loss: 1.0636, Test loss: 1.2013\n",
      "Epoch 45, Iteration 137000, Train loss: 1.0652, Test loss: 1.2002\n",
      "Epoch 45, Iteration 137100, Train loss: 1.0650, Test loss: 1.1963\n",
      "Epoch 45, Iteration 137200, Train loss: 1.0635, Test loss: 1.1922\n",
      "Epoch 45, Iteration 137300, Train loss: 1.0655, Test loss: 1.1954\n",
      "Epoch 45, Iteration 137400, Train loss: 1.0642, Test loss: 1.1980\n",
      "Epoch 45, Iteration 137500, Train loss: 1.0626, Test loss: 1.1989\n",
      "Epoch 45, Iteration 137600, Train loss: 1.0623, Test loss: 1.2008\n",
      "Epoch 45, Iteration 137700, Train loss: 1.0669, Test loss: 1.1926\n",
      "Epoch 45, Iteration 137800, Train loss: 1.0647, Test loss: 1.1999\n",
      "Epoch 45, Iteration 137900, Train loss: 1.0652, Test loss: 1.2002\n",
      "Epoch 45: Model saved\n",
      "Epoch 46: Model loaded\n",
      "Epoch 46, Iteration 138000, Train loss: 1.0636, Test loss: 1.2059\n",
      "Epoch 46, Iteration 138100, Train loss: 1.0622, Test loss: 1.2022\n",
      "Epoch 46, Iteration 138200, Train loss: 1.0624, Test loss: 1.1975\n",
      "Epoch 46, Iteration 138300, Train loss: 1.0671, Test loss: 1.1999\n",
      "Epoch 46, Iteration 138400, Train loss: 1.0656, Test loss: 1.2037\n",
      "Epoch 46, Iteration 138500, Train loss: 1.0627, Test loss: 1.1978\n",
      "Epoch 46, Iteration 138600, Train loss: 1.0625, Test loss: 1.1979\n",
      "Epoch 46, Iteration 138700, Train loss: 1.0613, Test loss: 1.1960\n",
      "Epoch 46, Iteration 138800, Train loss: 1.0633, Test loss: 1.2004\n",
      "Epoch 46, Iteration 138900, Train loss: 1.0620, Test loss: 1.1923\n",
      "Epoch 46, Iteration 139000, Train loss: 1.0661, Test loss: 1.1944\n",
      "Epoch 46, Iteration 139100, Train loss: 1.0668, Test loss: 1.1982\n",
      "Epoch 46, Iteration 139200, Train loss: 1.0633, Test loss: 1.2027\n",
      "Epoch 46, Iteration 139300, Train loss: 1.0673, Test loss: 1.2037\n",
      "Epoch 46, Iteration 139400, Train loss: 1.0645, Test loss: 1.2013\n",
      "Epoch 46, Iteration 139500, Train loss: 1.0649, Test loss: 1.2017\n",
      "Epoch 46, Iteration 139600, Train loss: 1.0638, Test loss: 1.1960\n",
      "Epoch 46, Iteration 139700, Train loss: 1.0594, Test loss: 1.1971\n",
      "Epoch 46, Iteration 139800, Train loss: 1.0627, Test loss: 1.2008\n",
      "Epoch 46, Iteration 139900, Train loss: 1.0664, Test loss: 1.1961\n",
      "Epoch 46, Iteration 140000, Train loss: 1.0612, Test loss: 1.1992\n",
      "Epoch 46, Iteration 140100, Train loss: 1.0609, Test loss: 1.1954\n",
      "Epoch 46, Iteration 140200, Train loss: 1.0620, Test loss: 1.1945\n",
      "Epoch 46, Iteration 140300, Train loss: 1.0640, Test loss: 1.1932\n",
      "Epoch 46, Iteration 140400, Train loss: 1.0594, Test loss: 1.1963\n",
      "Epoch 46, Iteration 140500, Train loss: 1.0616, Test loss: 1.1978\n",
      "Epoch 46, Iteration 140600, Train loss: 1.0594, Test loss: 1.1946\n",
      "Epoch 46, Iteration 140700, Train loss: 1.0616, Test loss: 1.1984\n",
      "Epoch 46, Iteration 140800, Train loss: 1.0617, Test loss: 1.1940\n",
      "Epoch 46, Iteration 140900, Train loss: 1.0638, Test loss: 1.1978\n",
      "Epoch 46: Model saved\n",
      "Epoch 47: Model loaded\n",
      "Epoch 47, Iteration 141000, Train loss: 1.0607, Test loss: 1.1943\n",
      "Epoch 47, Iteration 141100, Train loss: 1.0591, Test loss: 1.1964\n",
      "Epoch 47, Iteration 141200, Train loss: 1.0581, Test loss: 1.1962\n",
      "Epoch 47, Iteration 141300, Train loss: 1.0597, Test loss: 1.2000\n",
      "Epoch 47, Iteration 141400, Train loss: 1.0635, Test loss: 1.1975\n",
      "Epoch 47, Iteration 141500, Train loss: 1.0605, Test loss: 1.1965\n",
      "Epoch 47, Iteration 141600, Train loss: 1.0647, Test loss: 1.1940\n",
      "Epoch 47, Iteration 141700, Train loss: 1.0593, Test loss: 1.1999\n",
      "Epoch 47, Iteration 141800, Train loss: 1.0639, Test loss: 1.1999\n",
      "Epoch 47, Iteration 141900, Train loss: 1.0614, Test loss: 1.1966\n",
      "Epoch 47, Iteration 142000, Train loss: 1.0574, Test loss: 1.2008\n",
      "Epoch 47, Iteration 142100, Train loss: 1.0563, Test loss: 1.1965\n",
      "Epoch 47, Iteration 142200, Train loss: 1.0570, Test loss: 1.1929\n",
      "Epoch 47, Iteration 142300, Train loss: 1.0589, Test loss: 1.1978\n",
      "Epoch 47, Iteration 142400, Train loss: 1.0628, Test loss: 1.1988\n",
      "Epoch 47, Iteration 142500, Train loss: 1.0595, Test loss: 1.2008\n",
      "Epoch 47, Iteration 142600, Train loss: 1.0585, Test loss: 1.2000\n",
      "Epoch 47, Iteration 142700, Train loss: 1.0619, Test loss: 1.2022\n",
      "Epoch 47, Iteration 142800, Train loss: 1.0613, Test loss: 1.1879\n",
      "Epoch 47, Iteration 142900, Train loss: 1.0607, Test loss: 1.1977\n",
      "Epoch 47, Iteration 143000, Train loss: 1.0589, Test loss: 1.1931\n",
      "Epoch 47, Iteration 143100, Train loss: 1.0575, Test loss: 1.1972\n",
      "Epoch 47, Iteration 143200, Train loss: 1.0610, Test loss: 1.2007\n",
      "Epoch 47, Iteration 143300, Train loss: 1.0583, Test loss: 1.1948\n",
      "Epoch 47, Iteration 143400, Train loss: 1.0612, Test loss: 1.1928\n",
      "Epoch 47, Iteration 143500, Train loss: 1.0538, Test loss: 1.2040\n",
      "Epoch 47, Iteration 143600, Train loss: 1.0597, Test loss: 1.2042\n",
      "Epoch 47, Iteration 143700, Train loss: 1.0622, Test loss: 1.2040\n",
      "Epoch 47, Iteration 143800, Train loss: 1.0602, Test loss: 1.1880\n",
      "Epoch 47, Iteration 143900, Train loss: 1.0605, Test loss: 1.1963\n",
      "Epoch 47: Model saved\n",
      "Epoch 48: Model loaded\n",
      "Epoch 48, Iteration 144000, Train loss: 1.0590, Test loss: 1.1958\n",
      "Epoch 48, Iteration 144100, Train loss: 1.0592, Test loss: 1.1919\n",
      "Epoch 48, Iteration 144200, Train loss: 1.0638, Test loss: 1.1927\n",
      "Epoch 48, Iteration 144300, Train loss: 1.0629, Test loss: 1.1982\n",
      "Epoch 48, Iteration 144400, Train loss: 1.0565, Test loss: 1.1920\n",
      "Epoch 48, Iteration 144500, Train loss: 1.0580, Test loss: 1.1928\n",
      "Epoch 48, Iteration 144600, Train loss: 1.0592, Test loss: 1.1979\n",
      "Epoch 48, Iteration 144700, Train loss: 1.0586, Test loss: 1.1922\n",
      "Epoch 48, Iteration 144800, Train loss: 1.0558, Test loss: 1.1941\n",
      "Epoch 48, Iteration 144900, Train loss: 1.0596, Test loss: 1.2001\n",
      "Epoch 48, Iteration 145000, Train loss: 1.0556, Test loss: 1.1923\n",
      "Epoch 48, Iteration 145100, Train loss: 1.0589, Test loss: 1.1941\n",
      "Epoch 48, Iteration 145200, Train loss: 1.0572, Test loss: 1.1932\n",
      "Epoch 48, Iteration 145300, Train loss: 1.0608, Test loss: 1.1949\n",
      "Epoch 48, Iteration 145400, Train loss: 1.0599, Test loss: 1.2023\n",
      "Epoch 48, Iteration 145500, Train loss: 1.0581, Test loss: 1.1940\n",
      "Epoch 48, Iteration 145600, Train loss: 1.0530, Test loss: 1.1979\n",
      "Epoch 48, Iteration 145700, Train loss: 1.0584, Test loss: 1.1974\n",
      "Epoch 48, Iteration 145800, Train loss: 1.0566, Test loss: 1.1969\n",
      "Epoch 48, Iteration 145900, Train loss: 1.0558, Test loss: 1.1975\n",
      "Epoch 48, Iteration 146000, Train loss: 1.0585, Test loss: 1.1942\n",
      "Epoch 48, Iteration 146100, Train loss: 1.0564, Test loss: 1.1927\n",
      "Epoch 48, Iteration 146200, Train loss: 1.0587, Test loss: 1.1902\n",
      "Epoch 48, Iteration 146300, Train loss: 1.0546, Test loss: 1.1968\n",
      "Epoch 48, Iteration 146400, Train loss: 1.0552, Test loss: 1.1939\n",
      "Epoch 48, Iteration 146500, Train loss: 1.0548, Test loss: 1.1966\n",
      "Epoch 48, Iteration 146600, Train loss: 1.0553, Test loss: 1.1908\n",
      "Epoch 48, Iteration 146700, Train loss: 1.0581, Test loss: 1.1942\n",
      "Epoch 48, Iteration 146800, Train loss: 1.0583, Test loss: 1.1931\n",
      "Epoch 48, Iteration 146900, Train loss: 1.0557, Test loss: 1.1963\n",
      "Epoch 48: Model saved\n",
      "Epoch 49: Model loaded\n",
      "Epoch 49, Iteration 147000, Train loss: 1.0562, Test loss: 1.2009\n",
      "Epoch 49, Iteration 147100, Train loss: 1.0581, Test loss: 1.1914\n",
      "Epoch 49, Iteration 147200, Train loss: 1.0568, Test loss: 1.1942\n",
      "Epoch 49, Iteration 147300, Train loss: 1.0542, Test loss: 1.2021\n",
      "Epoch 49, Iteration 147400, Train loss: 1.0586, Test loss: 1.1986\n",
      "Epoch 49, Iteration 147500, Train loss: 1.0569, Test loss: 1.1974\n",
      "Epoch 49, Iteration 147600, Train loss: 1.0606, Test loss: 1.2039\n",
      "Epoch 49, Iteration 147700, Train loss: 1.0551, Test loss: 1.1911\n",
      "Epoch 49, Iteration 147800, Train loss: 1.0536, Test loss: 1.1919\n",
      "Epoch 49, Iteration 147900, Train loss: 1.0543, Test loss: 1.1963\n",
      "Epoch 49, Iteration 148000, Train loss: 1.0573, Test loss: 1.1952\n",
      "Epoch 49, Iteration 148100, Train loss: 1.0541, Test loss: 1.1949\n",
      "Epoch 49, Iteration 148200, Train loss: 1.0538, Test loss: 1.1904\n",
      "Epoch 49, Iteration 148300, Train loss: 1.0570, Test loss: 1.1984\n",
      "Epoch 49, Iteration 148400, Train loss: 1.0571, Test loss: 1.1958\n",
      "Epoch 49, Iteration 148500, Train loss: 1.0571, Test loss: 1.1957\n",
      "Epoch 49, Iteration 148600, Train loss: 1.0535, Test loss: 1.1956\n",
      "Epoch 49, Iteration 148700, Train loss: 1.0545, Test loss: 1.1948\n",
      "Epoch 49, Iteration 148800, Train loss: 1.0547, Test loss: 1.1925\n",
      "Epoch 49, Iteration 148900, Train loss: 1.0529, Test loss: 1.1949\n",
      "Epoch 49, Iteration 149000, Train loss: 1.0531, Test loss: 1.1970\n",
      "Epoch 49, Iteration 149100, Train loss: 1.0527, Test loss: 1.1904\n",
      "Epoch 49, Iteration 149200, Train loss: 1.0529, Test loss: 1.1912\n",
      "Epoch 49, Iteration 149300, Train loss: 1.0563, Test loss: 1.1931\n",
      "Epoch 49, Iteration 149400, Train loss: 1.0545, Test loss: 1.1932\n",
      "Epoch 49, Iteration 149500, Train loss: 1.0542, Test loss: 1.1979\n",
      "Epoch 49, Iteration 149600, Train loss: 1.0568, Test loss: 1.1947\n",
      "Epoch 49, Iteration 149700, Train loss: 1.0535, Test loss: 1.1967\n",
      "Epoch 49, Iteration 149800, Train loss: 1.0543, Test loss: 1.1963\n",
      "Epoch 49, Iteration 149900, Train loss: 1.0534, Test loss: 1.1902\n",
      "Epoch 49: Model saved\n",
      "Final loss: 1.1170015335083008\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "vocab_size = len(chars)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(epoch):\n",
    "    if i > 0 and os.path.exists('lenisha.pkl'):\n",
    "        with open('lenisha.pkl', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            print(f\"Epoch {i}: Model loaded\")\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        if i == 0:\n",
    "            print(\"Epoch 0: Starting with fresh model\")\n",
    "\n",
    "    for itters in range(max_itters):\n",
    "        if itters % eval_itters == 0:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"Epoch {i}, Iteration {i * max_itters + itters}, Train loss: {losses['train']:.4f}, Test loss: {losses['test']:.4f}\")\n",
    "        x, y = get_batch(\"train\")\n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with open('lenisha.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "        print(f\"Epoch {i}: Model saved\")\n",
    "\n",
    "print(\"Final loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor(encode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy name is lenisha  \u001b[39m\u001b[38;5;124m\"\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m      2\u001b[0m generated_chars \u001b[38;5;241m=\u001b[39m decode(model\u001b[38;5;241m.\u001b[39mgenerate(context\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m500\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_chars)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "context = torch.tensor(encode(\"my name is lenisha  \"), dtype=torch.long, device=device)\n",
    "generated_chars = decode(model.generate(context.unsqueeze(0), 500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backed up lenisha.pkl\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.copyfile(\"lenisha.pkl\", \"lenisha_backup.pkl\")\n",
    "print(\"Backed up lenisha.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated lenisha.pkl with model and vocabulary\n",
      "Loaded keys: dict_keys(['model', 'chars'])\n",
      "Number of characters in vocab: 1118\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "train_data = data[\"train\"]['text']\n",
    "test_data = data[\"validation\"]['text']\n",
    "train_text = \"\".join(train_data)\n",
    "test_text = \"\".join(test_data)\n",
    "chars = sorted(list(set(train_text + test_text)))\n",
    "\n",
    "with open('lenisha.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open('lenisha.pkl', 'wb') as f:\n",
    "    pickle.dump({\"model\": model, \"chars\": chars}, f)\n",
    "    print(\"Updated lenisha.pkl with model and vocabulary\")\n",
    "\n",
    "with open('lenisha.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    print(\"Loaded keys:\", data.keys())\n",
    "    print(\"Number of characters in vocab:\", len(data[\"chars\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
